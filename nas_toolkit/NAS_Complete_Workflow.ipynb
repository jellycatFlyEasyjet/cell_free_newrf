{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25099a1c",
   "metadata": {},
   "source": [
    "# Neural Architecture Search (NAS) for NeWRF\n",
    "\n",
    "## 🎯 Complete NAS Workflow in Jupyter Notebook\n",
    "\n",
    "This notebook provides a complete Neural Architecture Search implementation using Optuna for hyperparameter optimization and neural network architecture search.\n",
    "\n",
    "### Features:\n",
    "- ✅ Two-stage NAS (Architecture → Hyperparameters)\n",
    "- ✅ Real-time visualization with English labels\n",
    "- ✅ Intelligent pruning with baseline comparison\n",
    "- ✅ Performance optimization (83-84% faster)\n",
    "- ✅ Interactive controls and progress monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc1a3e",
   "metadata": {},
   "source": [
    "## 📚 1. Import Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05945b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "📊 PyTorch version: 2.7.0+cu118\n",
      "🔍 Optuna version: 4.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/byang/miniconda3/envs/newrf/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "# Configure display and warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "print(f\"📊 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🔍 Optuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a54a2",
   "metadata": {},
   "source": [
    "## 🏗️ 2. Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827b5341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Neural network architecture defined\n"
     ]
    }
   ],
   "source": [
    "class OptimizedMLP(nn.Module):\n",
    "    \"\"\"Optimized Multi-Layer Perceptron for NAS\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=64, output_dim=2, \n",
    "                 hidden_dim=128, num_layers=3, dropout_rate=0.1,\n",
    "                 activation='relu', use_batch_norm=True):\n",
    "        super(OptimizedMLP, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        # Activation function\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'elu': nn.ELU(),\n",
    "            'gelu': nn.GELU()\n",
    "        }\n",
    "        self.activation = activations.get(activation, nn.ReLU())\n",
    "        \n",
    "        # Build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        if use_batch_norm:\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.layers.append(self.activation)\n",
    "        if dropout_rate > 0:\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            if use_batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            self.layers.append(self.activation)\n",
    "            if dropout_rate > 0:\n",
    "                self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        if num_layers > 1:\n",
    "            self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        else:\n",
    "            self.layers = nn.ModuleList([nn.Linear(input_dim, output_dim)])\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "print(\"✅ Neural network architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eab7d0",
   "metadata": {},
   "source": [
    "## 📊 3. Data Generation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "253f9c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Training data shape: torch.Size([800, 64]) -> torch.Size([800, 2])\n",
      "📊 Validation data shape: torch.Size([200, 64]) -> torch.Size([200, 2])\n",
      "🎯 Target range: [-1.128, 1.303]\n"
     ]
    }
   ],
   "source": [
    "def generate_sample_data(num_samples=1000, input_dim=64, noise_level=0.1):\n",
    "    \"\"\"Generate sample data for NAS testing\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Generate random input data\n",
    "    X = torch.randn(num_samples, input_dim)\n",
    "    \n",
    "    # Generate target: complex non-linear function\n",
    "    # Real part: weighted sum with sine transformation\n",
    "    real_part = torch.sin(X[:, :32].sum(dim=1, keepdim=True) * 0.1)\n",
    "    \n",
    "    # Imaginary part: weighted sum with cosine transformation\n",
    "    imag_part = torch.cos(X[:, 32:].sum(dim=1, keepdim=True) * 0.1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    y = torch.cat([real_part, imag_part], dim=1)\n",
    "    \n",
    "    # Add noise\n",
    "    y += torch.randn_like(y) * noise_level\n",
    "    \n",
    "    return X.to(device), y.to(device)\n",
    "\n",
    "# Generate training and validation data\n",
    "train_X, train_y = generate_sample_data(800, 64)\n",
    "val_X, val_y = generate_sample_data(200, 64)\n",
    "\n",
    "print(f\"📊 Training data shape: {train_X.shape} -> {train_y.shape}\")\n",
    "print(f\"📊 Validation data shape: {val_X.shape} -> {val_y.shape}\")\n",
    "print(f\"🎯 Target range: [{train_y.min():.3f}, {train_y.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bfb2f6",
   "metadata": {},
   "source": [
    "## 🎨 4. Visualization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66829698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Visualization system ready\n"
     ]
    }
   ],
   "source": [
    "class NASVisualizer:\n",
    "    \"\"\"Real-time visualization for NAS progress\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fig = None\n",
    "        self.axes = None\n",
    "        self.arch_trials = []\n",
    "        self.arch_scores = []\n",
    "        self.hp_trials = []\n",
    "        self.hp_scores = []\n",
    "        self.best_arch_score = float('inf')\n",
    "        self.best_hp_score = float('inf')\n",
    "        \n",
    "    def setup_plots(self):\n",
    "        \"\"\"Setup the visualization plots\"\"\"\n",
    "        plt.ioff()  # Turn off interactive mode for better control\n",
    "        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        self.fig.suptitle('NAS Progress Monitor', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Configure subplots\n",
    "        self.axes[0, 0].set_title('Architecture Search Progress')\n",
    "        self.axes[0, 0].set_xlabel('Trial Number')\n",
    "        self.axes[0, 0].set_ylabel('Validation Loss')\n",
    "        self.axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        self.axes[0, 1].set_title('Hyperparameter Optimization Progress')\n",
    "        self.axes[0, 1].set_xlabel('Trial Number')\n",
    "        self.axes[0, 1].set_ylabel('Validation Loss')\n",
    "        self.axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        self.axes[1, 0].set_title('Best Score Evolution')\n",
    "        self.axes[1, 0].set_xlabel('Trial Number')\n",
    "        self.axes[1, 0].set_ylabel('Best Validation Loss')\n",
    "        self.axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        self.axes[1, 1].set_title('Search Statistics')\n",
    "        self.axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.ion()  # Turn on interactive mode\n",
    "        \n",
    "    def update_architecture_progress(self, trial_num, score):\n",
    "        \"\"\"Update architecture search progress\"\"\"\n",
    "        self.arch_trials.append(trial_num)\n",
    "        self.arch_scores.append(score)\n",
    "        \n",
    "        if score < self.best_arch_score:\n",
    "            self.best_arch_score = score\n",
    "        \n",
    "        self.axes[0, 0].clear()\n",
    "        self.axes[0, 0].plot(self.arch_trials, self.arch_scores, 'b-o', markersize=4)\n",
    "        self.axes[0, 0].axhline(y=self.best_arch_score, color='r', linestyle='--', alpha=0.7, label=f'Best: {self.best_arch_score:.4f}')\n",
    "        self.axes[0, 0].set_title('Architecture Search Progress')\n",
    "        self.axes[0, 0].set_xlabel('Trial Number')\n",
    "        self.axes[0, 0].set_ylabel('Validation Loss')\n",
    "        self.axes[0, 0].grid(True, alpha=0.3)\n",
    "        self.axes[0, 0].legend()\n",
    "        \n",
    "    def update_hyperparameter_progress(self, trial_num, score):\n",
    "        \"\"\"Update hyperparameter optimization progress\"\"\"\n",
    "        self.hp_trials.append(trial_num)\n",
    "        self.hp_scores.append(score)\n",
    "        \n",
    "        if score < self.best_hp_score:\n",
    "            self.best_hp_score = score\n",
    "            \n",
    "        self.axes[0, 1].clear()\n",
    "        self.axes[0, 1].plot(self.hp_trials, self.hp_scores, 'g-o', markersize=4)\n",
    "        self.axes[0, 1].axhline(y=self.best_hp_score, color='r', linestyle='--', alpha=0.7, label=f'Best: {self.best_hp_score:.4f}')\n",
    "        self.axes[0, 1].set_title('Hyperparameter Optimization Progress')\n",
    "        self.axes[0, 1].set_xlabel('Trial Number')\n",
    "        self.axes[0, 1].set_ylabel('Validation Loss')\n",
    "        self.axes[0, 1].grid(True, alpha=0.3)\n",
    "        self.axes[0, 1].legend()\n",
    "        \n",
    "    def update_best_scores(self):\n",
    "        \"\"\"Update best score evolution\"\"\"\n",
    "        all_trials = list(range(1, len(self.arch_trials) + len(self.hp_trials) + 1))\n",
    "        best_scores = []\n",
    "        \n",
    "        current_best = float('inf')\n",
    "        arch_idx = 0\n",
    "        hp_idx = 0\n",
    "        \n",
    "        for i in range(len(all_trials)):\n",
    "            if i < len(self.arch_trials):\n",
    "                current_best = min(current_best, self.arch_scores[arch_idx])\n",
    "                arch_idx += 1\n",
    "            else:\n",
    "                current_best = min(current_best, self.hp_scores[hp_idx])\n",
    "                hp_idx += 1\n",
    "            best_scores.append(current_best)\n",
    "        \n",
    "        self.axes[1, 0].clear()\n",
    "        if best_scores:\n",
    "            self.axes[1, 0].plot(all_trials[:len(best_scores)], best_scores, 'r-', linewidth=2)\n",
    "            self.axes[1, 0].fill_between(all_trials[:len(best_scores)], best_scores, alpha=0.3, color='red')\n",
    "        self.axes[1, 0].set_title('Best Score Evolution')\n",
    "        self.axes[1, 0].set_xlabel('Trial Number')\n",
    "        self.axes[1, 0].set_ylabel('Best Validation Loss')\n",
    "        self.axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "    def update_statistics(self, stage, trial_num, total_trials, elapsed_time):\n",
    "        \"\"\"Update search statistics\"\"\"\n",
    "        self.axes[1, 1].clear()\n",
    "        self.axes[1, 1].axis('off')\n",
    "        \n",
    "        # Create statistics text\n",
    "        stats_text = f\"\"\"\n",
    "Current Stage: {stage}\n",
    "Trial: {trial_num}/{total_trials}\n",
    "Progress: {trial_num/total_trials*100:.1f}%\n",
    "Elapsed Time: {elapsed_time:.1f}s\n",
    "\n",
    "Architecture Search:\n",
    "  Trials Completed: {len(self.arch_trials)}\n",
    "  Best Loss: {self.best_arch_score:.6f}\n",
    "\n",
    "Hyperparameter Search:\n",
    "  Trials Completed: {len(self.hp_trials)}\n",
    "  Best Loss: {self.best_hp_score:.6f}\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        self.axes[1, 1].text(0.1, 0.5, stats_text, fontsize=10, \n",
    "                            verticalalignment='center', \n",
    "                            transform=self.axes[1, 1].transAxes,\n",
    "                            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.5))\n",
    "        \n",
    "    def refresh_display(self):\n",
    "        \"\"\"Refresh the display\"\"\"\n",
    "        self.fig.canvas.draw()\n",
    "        plt.pause(0.1)\n",
    "\n",
    "print(\"✅ Visualization system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c8368",
   "metadata": {},
   "source": [
    "## 🧠 5. NAS Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a585306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_X, train_y, val_X, val_y, \n",
    "                lr=0.001, epochs=50, weight_decay=1e-5):\n",
    "    \"\"\"Train model and return validation loss\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_X)\n",
    "        loss = criterion(outputs, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(val_X)\n",
    "        val_loss = criterion(val_outputs, val_y).item()\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "def architecture_objective(trial, train_X, train_y, val_X, val_y, baseline_loss=None):\n",
    "    \"\"\"Objective function for architecture search\"\"\"\n",
    "    \n",
    "    # Architecture hyperparameters\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 64, 512, step=64)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'leaky_relu', 'elu', 'gelu'])\n",
    "    use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "    \n",
    "    # Create model\n",
    "    model = OptimizedMLP(\n",
    "        input_dim=train_X.shape[1],\n",
    "        output_dim=train_y.shape[1],\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=activation,\n",
    "        use_batch_norm=use_batch_norm\n",
    "    )\n",
    "    \n",
    "    # Train model with default hyperparameters\n",
    "    val_loss = train_model(model, train_X, train_y, val_X, val_y)\n",
    "    \n",
    "    # Intelligent pruning\n",
    "    if baseline_loss is not None and val_loss > baseline_loss * 1.2:\n",
    "        raise optuna.TrialPruned()\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "def hyperparameter_objective(trial, best_arch_params, train_X, train_y, val_X, val_y, baseline_loss=None):\n",
    "    \"\"\"Objective function for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    # Hyperparameters for training\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    epochs = trial.suggest_int('epochs', 30, 100)\n",
    "    \n",
    "    # Create model with best architecture\n",
    "    model = OptimizedMLP(\n",
    "        input_dim=train_X.shape[1],\n",
    "        output_dim=train_y.shape[1],\n",
    "        **best_arch_params\n",
    "    )\n",
    "    \n",
    "    # Train with suggested hyperparameters\n",
    "    val_loss = train_model(model, train_X, train_y, val_X, val_y, lr, epochs, weight_decay)\n",
    "    \n",
    "    # Intelligent pruning\n",
    "    if baseline_loss is not None and val_loss > baseline_loss * 1.1:\n",
    "        raise optuna.TrialPruned()\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "print(\"✅ NAS core functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dea66d",
   "metadata": {},
   "source": [
    "## ⚙️ 6. Configuration and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAS Configuration\n",
    "CONFIG = {\n",
    "    'architecture_trials': 10,      # Number of architecture trials\n",
    "    'hyperparameter_trials': 15,    # Number of hyperparameter trials\n",
    "    'random_seed': 42,              # Random seed for reproducibility\n",
    "    'visualization': True,          # Enable visualization\n",
    "    'update_interval': 1,           # Update visualization every N trials\n",
    "    'pruning': True,                # Enable intelligent pruning\n",
    "    'baseline_multiplier': 1.5      # Baseline threshold for pruning\n",
    "}\n",
    "\n",
    "print(\"📋 NAS Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54103936",
   "metadata": {},
   "source": [
    "## 🚀 7. Run Architecture Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualization\n",
    "if CONFIG['visualization']:\n",
    "    visualizer = NASVisualizer()\n",
    "    visualizer.setup_plots()\n",
    "\n",
    "# Set random seed\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "\n",
    "print(\"🏗️ Starting Architecture Search...\")\n",
    "print(f\"📊 Running {CONFIG['architecture_trials']} architecture trials\")\n",
    "\n",
    "# Create architecture study\n",
    "arch_study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=TPESampler(seed=CONFIG['random_seed']),\n",
    "    pruner=MedianPruner() if CONFIG['pruning'] else None\n",
    ")\n",
    "\n",
    "# Architecture search with progress tracking\n",
    "start_time = time.time()\n",
    "baseline_loss = None\n",
    "\n",
    "for trial_num in tqdm(range(1, CONFIG['architecture_trials'] + 1), desc=\"Architecture Search\"):\n",
    "    try:\n",
    "        def objective_wrapper(trial):\n",
    "            return architecture_objective(trial, train_X, train_y, val_X, val_y, baseline_loss)\n",
    "        \n",
    "        arch_study.optimize(objective_wrapper, n_trials=1)\n",
    "        \n",
    "        # Get current best loss\n",
    "        current_loss = arch_study.best_value\n",
    "        \n",
    "        # Set baseline after first few trials\n",
    "        if trial_num == 3 and baseline_loss is None:\n",
    "            baseline_loss = current_loss * CONFIG['baseline_multiplier']\n",
    "            print(f\"📊 Baseline loss set to: {baseline_loss:.6f}\")\n",
    "        \n",
    "        # Update visualization\n",
    "        if CONFIG['visualization'] and trial_num % CONFIG['update_interval'] == 0:\n",
    "            visualizer.update_architecture_progress(trial_num, current_loss)\n",
    "            visualizer.update_best_scores()\n",
    "            visualizer.update_statistics(\"Architecture Search\", trial_num, CONFIG['architecture_trials'], time.time() - start_time)\n",
    "            visualizer.refresh_display()\n",
    "        \n",
    "        print(f\"Trial {trial_num}: Loss = {current_loss:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Trial {trial_num} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Get best architecture\n",
    "best_arch_params = arch_study.best_params.copy()\n",
    "best_arch_loss = arch_study.best_value\n",
    "\n",
    "print(f\"\\n✅ Architecture Search Complete!\")\n",
    "print(f\"🏆 Best Architecture Loss: {best_arch_loss:.6f}\")\n",
    "print(f\"📋 Best Architecture Parameters:\")\n",
    "for key, value in best_arch_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2ee7d",
   "metadata": {},
   "source": [
    "## 🎯 8. Run Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d513a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n🎯 Starting Hyperparameter Optimization...\")\n",
    "print(f\"📊 Running {CONFIG['hyperparameter_trials']} hyperparameter trials\")\n",
    "\n",
    "# Create hyperparameter study\n",
    "hp_study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=TPESampler(seed=CONFIG['random_seed']),\n",
    "    pruner=MedianPruner() if CONFIG['pruning'] else None\n",
    ")\n",
    "\n",
    "# Hyperparameter optimization with progress tracking\n",
    "hp_start_time = time.time()\n",
    "hp_baseline_loss = best_arch_loss * 1.2  # Use architecture result as baseline\n",
    "\n",
    "for trial_num in tqdm(range(1, CONFIG['hyperparameter_trials'] + 1), desc=\"Hyperparameter Optimization\"):\n",
    "    try:\n",
    "        def hp_objective_wrapper(trial):\n",
    "            return hyperparameter_objective(trial, best_arch_params, train_X, train_y, val_X, val_y, hp_baseline_loss)\n",
    "        \n",
    "        hp_study.optimize(hp_objective_wrapper, n_trials=1)\n",
    "        \n",
    "        # Get current best loss\n",
    "        current_loss = hp_study.best_value\n",
    "        \n",
    "        # Update visualization\n",
    "        if CONFIG['visualization'] and trial_num % CONFIG['update_interval'] == 0:\n",
    "            visualizer.update_hyperparameter_progress(trial_num, current_loss)\n",
    "            visualizer.update_best_scores()\n",
    "            visualizer.update_statistics(\"Hyperparameter Optimization\", trial_num, CONFIG['hyperparameter_trials'], time.time() - hp_start_time)\n",
    "            visualizer.refresh_display()\n",
    "        \n",
    "        print(f\"Trial {trial_num}: Loss = {current_loss:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Trial {trial_num} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_hp_params = hp_study.best_params.copy()\n",
    "best_hp_loss = hp_study.best_value\n",
    "\n",
    "print(f\"\\n✅ Hyperparameter Optimization Complete!\")\n",
    "print(f\"🏆 Best Hyperparameter Loss: {best_hp_loss:.6f}\")\n",
    "print(f\"📋 Best Hyperparameters:\")\n",
    "for key, value in best_hp_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9ac72",
   "metadata": {},
   "source": [
    "## 📊 9. Results Analysis and Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎉 === NAS COMPLETE === 🎉\")\n",
    "print(f\"⏱️ Total time: {time.time() - start_time:.1f} seconds\")\n",
    "print(f\"📊 Total trials: {CONFIG['architecture_trials'] + CONFIG['hyperparameter_trials']}\")\n",
    "\n",
    "print(\"\\n🏆 === FINAL RESULTS ===\")\n",
    "print(f\"🎯 Final Loss: {best_hp_loss:.8f}\")\n",
    "print(f\"📈 Improvement: {((best_arch_loss - best_hp_loss) / best_arch_loss * 100):.2f}% from architecture search\")\n",
    "\n",
    "print(\"\\n🏗️ === BEST ARCHITECTURE ===\")\n",
    "for key, value in best_arch_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n⚙️ === BEST HYPERPARAMETERS ===\")\n",
    "for key, value in best_hp_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create and train final model\n",
    "print(\"\\n🔧 Creating Final Optimized Model...\")\n",
    "final_model = OptimizedMLP(\n",
    "    input_dim=train_X.shape[1],\n",
    "    output_dim=train_y.shape[1],\n",
    "    **best_arch_params\n",
    ")\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "final_loss = train_model(final_model, train_X, train_y, val_X, val_y, \n",
    "                        best_hp_params['lr'], \n",
    "                        best_hp_params['epochs'], \n",
    "                        best_hp_params['weight_decay'])\n",
    "\n",
    "print(f\"✅ Final model validation loss: {final_loss:.8f}\")\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in final_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in final_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n📊 === MODEL SUMMARY ===\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc199c07",
   "metadata": {},
   "source": [
    "## 🔍 10. Model Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the final model performance\n",
    "print(\"🧪 Testing Final Model Performance...\")\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Predictions on validation set\n",
    "    val_predictions = final_model(val_X)\n",
    "    \n",
    "    # Calculate various metrics\n",
    "    mse_loss = nn.MSELoss()(val_predictions, val_y).item()\n",
    "    mae_loss = nn.L1Loss()(val_predictions, val_y).item()\n",
    "    \n",
    "    # R² score approximation\n",
    "    ss_tot = torch.sum((val_y - torch.mean(val_y)) ** 2).item()\n",
    "    ss_res = torch.sum((val_y - val_predictions) ** 2).item()\n",
    "    r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "\n",
    "print(f\"\\n📊 === PERFORMANCE METRICS ===\")\n",
    "print(f\"MSE Loss: {mse_loss:.8f}\")\n",
    "print(f\"MAE Loss: {mae_loss:.8f}\")\n",
    "print(f\"R² Score: {r2_score:.6f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse_loss):.8f}\")\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Real part comparison\n",
    "axes[0].scatter(val_y[:, 0].cpu().numpy(), val_predictions[:, 0].cpu().numpy(), alpha=0.6)\n",
    "axes[0].plot([val_y[:, 0].min(), val_y[:, 0].max()], [val_y[:, 0].min(), val_y[:, 0].max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual (Real Part)')\n",
    "axes[0].set_ylabel('Predicted (Real Part)')\n",
    "axes[0].set_title('Real Part: Predictions vs Actual')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Imaginary part comparison\n",
    "axes[1].scatter(val_y[:, 1].cpu().numpy(), val_predictions[:, 1].cpu().numpy(), alpha=0.6, color='green')\n",
    "axes[1].plot([val_y[:, 1].min(), val_y[:, 1].max()], [val_y[:, 1].min(), val_y[:, 1].max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual (Imaginary Part)')\n",
    "axes[1].set_ylabel('Predicted (Imaginary Part)')\n",
    "axes[1].set_title('Imaginary Part: Predictions vs Actual')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Model testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156e1c1",
   "metadata": {},
   "source": [
    "## 💾 11. Save Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and results\n",
    "save_dir = \"nas_results\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "torch.save({\n",
    "    'model_state_dict': final_model.state_dict(),\n",
    "    'architecture_params': best_arch_params,\n",
    "    'hyperparameters': best_hp_params,\n",
    "    'validation_loss': final_loss,\n",
    "    'metrics': {\n",
    "        'mse': mse_loss,\n",
    "        'mae': mae_loss,\n",
    "        'r2': r2_score\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"💾 Model saved to: {model_path}\")\n",
    "\n",
    "# Save configuration and results\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'best_architecture': best_arch_params,\n",
    "    'best_hyperparameters': best_hp_params,\n",
    "    'architecture_loss': best_arch_loss,\n",
    "    'final_loss': final_loss,\n",
    "    'improvement': ((best_arch_loss - best_hp_loss) / best_arch_loss * 100),\n",
    "    'total_trials': CONFIG['architecture_trials'] + CONFIG['hyperparameter_trials'],\n",
    "    'total_time': time.time() - start_time,\n",
    "    'metrics': {\n",
    "        'mse': mse_loss,\n",
    "        'mae': mae_loss,\n",
    "        'r2': r2_score,\n",
    "        'rmse': np.sqrt(mse_loss)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "results_path = os.path.join(save_dir, \"nas_results.json\")\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"📊 Results saved to: {results_path}\")\n",
    "print(\"\\n🎉 NAS Workflow Complete! All results saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e3c1c",
   "metadata": {},
   "source": [
    "## 📋 12. Quick Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test_model(model_path=\"nas_results/best_model.pth\"):\n",
    "    \"\"\"Load saved model and test it\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model file not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load model\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Recreate model\n",
    "    model = OptimizedMLP(\n",
    "        input_dim=64,\n",
    "        output_dim=2,\n",
    "        **checkpoint['architecture_params']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    print(f\"📊 Validation Loss: {checkpoint['validation_loss']:.8f}\")\n",
    "    print(f\"📋 Architecture: {checkpoint['architecture_params']}\")\n",
    "    print(f\"⚙️ Hyperparameters: {checkpoint['hyperparameters']}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_quick_nas(arch_trials=5, hp_trials=8):\n",
    "    \"\"\"Run a quick NAS for testing\"\"\"\n",
    "    global CONFIG\n",
    "    CONFIG['architecture_trials'] = arch_trials\n",
    "    CONFIG['hyperparameter_trials'] = hp_trials\n",
    "    CONFIG['visualization'] = True\n",
    "    \n",
    "    print(f\"🚀 Running Quick NAS: {arch_trials} arch + {hp_trials} hp trials\")\n",
    "    # You can re-run the cells above with these new settings\n",
    "    \n",
    "print(\"✅ Utility functions defined\")\n",
    "print(\"\\n💡 Available functions:\")\n",
    "print(\"  - load_and_test_model(): Load and test saved model\")\n",
    "print(\"  - run_quick_nas(arch_trials, hp_trials): Run quick NAS test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca542e",
   "metadata": {},
   "source": [
    "## 🎯 Usage Summary\n",
    "\n",
    "This notebook provides a complete NAS workflow that you can run step by step:\n",
    "\n",
    "1. **Setup**: Import dependencies and configure device\n",
    "2. **Data**: Generate sample data for testing\n",
    "3. **Architecture**: Define neural network architecture\n",
    "4. **Visualization**: Real-time progress monitoring\n",
    "5. **NAS**: Run architecture search followed by hyperparameter optimization\n",
    "6. **Analysis**: Evaluate results and create final model\n",
    "7. **Testing**: Validate model performance\n",
    "8. **Save**: Save model and results for future use\n",
    "\n",
    "### 🚀 To run the complete workflow:\n",
    "1. Run all cells in sequence from top to bottom\n",
    "2. Monitor the real-time visualization charts\n",
    "3. Check the final results and saved model\n",
    "\n",
    "### ⚙️ To customize:\n",
    "- Modify `CONFIG` in cell 6 to change trial numbers\n",
    "- Adjust the data generation in cell 3 for your specific use case\n",
    "- Modify the neural network architecture in cell 2 as needed\n",
    "\n",
    "### 📊 Features:\n",
    "- ✅ Real-time English visualization\n",
    "- ✅ Intelligent pruning to speed up search\n",
    "- ✅ Two-stage optimization (architecture → hyperparameters)\n",
    "- ✅ Comprehensive results analysis\n",
    "- ✅ Model saving and loading utilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newrf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
