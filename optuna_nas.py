import optuna
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import pickle
from tqdm import trange, tqdm
import logging
import time
from torch.optim.lr_scheduler import ReduceLROnPlateau
from loaders import DatasetLoader
from config_function import *
from models import MLP  # å¯¼å…¥baselineæ¨¡å‹
import random
import matplotlib.pyplot as plt
from datetime import datetime
import threading

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')

# å…¨å±€å˜é‡å­˜å‚¨baselineæ€§èƒ½
BASELINE_PERFORMANCE = None

# å…¨å±€å˜é‡ç”¨äºè¿›åº¦å¯è§†åŒ–
class ProgressTracker:
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.baseline_progress = 0
        self.arch_trials_completed = 0
        self.arch_trials_total = 0
        self.arch_best_loss = float('inf')
        self.hp_trials_completed = 0
        self.hp_trials_total = 0
        self.hp_best_loss = float('inf')
        self.current_stage = "Preparing"
        self.stage_progress = 0
        self.start_time = time.time()
        self.arch_losses = []
        self.hp_losses = []
        self.baseline_loss = None
    
    def update_baseline(self, progress):
        self.baseline_progress = progress
        self.current_stage = f"Baseline Evaluation ({progress}%)"
    
    def set_baseline_complete(self, loss):
        self.baseline_loss = loss
        self.current_stage = "Baseline Complete, Preparing Architecture Search"
    
    def set_arch_stage(self, total_trials):
        self.arch_trials_total = total_trials
        self.current_stage = "Architecture Search in Progress"
    
    def update_arch_trial(self, completed, best_loss):
        self.arch_trials_completed = completed
        if best_loss < self.arch_best_loss:
            self.arch_best_loss = best_loss
        self.arch_losses.append(best_loss)
        progress = int((completed / self.arch_trials_total) * 100) if self.arch_trials_total > 0 else 0
        self.current_stage = f"Architecture Search ({completed}/{self.arch_trials_total}) - {progress}%"
    
    def set_hp_stage(self, total_trials):
        self.hp_trials_total = total_trials
        self.current_stage = "Hyperparameter Search in Progress"
    
    def update_hp_trial(self, completed, best_loss):
        self.hp_trials_completed = completed
        if best_loss < self.hp_best_loss:
            self.hp_best_loss = best_loss
        self.hp_losses.append(best_loss)
        progress = int((completed / self.hp_trials_total) * 100) if self.hp_trials_total > 0 else 0
        self.current_stage = f"Hyperparameter Search ({completed}/{self.hp_trials_total}) - {progress}%"
    
    def get_elapsed_time(self):
        return time.time() - self.start_time
    
    def print_status(self):
        elapsed = self.get_elapsed_time()
        print(f"\n{'='*60}")
        print(f"ğŸš€ NAS Progress Status | Elapsed: {elapsed/60:.1f} minutes")
        print(f"Current Stage: {self.current_stage}")
        
        if self.baseline_loss:
            print(f"ğŸ“Š Baseline Performance: Loss={self.baseline_loss:.6f}")
        
        if self.arch_trials_completed > 0:
            arch_progress = (self.arch_trials_completed / self.arch_trials_total * 100) if self.arch_trials_total > 0 else 0
            print(f"ğŸ—ï¸  Architecture Search: {self.arch_trials_completed}/{self.arch_trials_total} ({arch_progress:.0f}%) | Best Loss: {self.arch_best_loss:.6f}")
        
        if self.hp_trials_completed > 0:
            hp_progress = (self.hp_trials_completed / self.hp_trials_total * 100) if self.hp_trials_total > 0 else 0
            print(f"âš¡ Hyperparameter Search: {self.hp_trials_completed}/{self.hp_trials_total} ({hp_progress:.0f}%) | Best Loss: {self.hp_best_loss:.6f}")
        
        print(f"{'='*60}")

# å…¨å±€è¿›åº¦è¿½è¸ªå™¨
progress_tracker = ProgressTracker()


def evaluate_baseline_model():
    """è¯„ä¼°baselineæ¨¡å‹æ€§èƒ½"""
    global BASELINE_PERFORMANCE
    
    if BASELINE_PERFORMANCE is not None:
        return BASELINE_PERFORMANCE
    
    logging.info("ğŸ“ Evaluating Baseline model performance...")
    progress_tracker.current_stage = "Baseline Model Evaluation in Progress"
    
    try:
        # ä½¿ç”¨ä¸NASç›¸åŒçš„è®¾ç½®
        dataset_fname = '/home/byang/BoYang/mNeWRF/dataset/conference_2000STA_4APs.pkl'
        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        
        # å›ºå®šéšæœºç§å­
        torch.manual_seed(42)
        np.random.seed(42)
        random.seed(42)
        
        # åˆ›å»ºbaselineæ¨¡å‹
        baseline_model = MLP()
        baseline_model.to(device)
        
        # ä½¿ç”¨å›ºå®šçš„è®­ç»ƒé…ç½®
        learning_rate = 5e-4
        weight_decay = 0
        batch_size = 1024
        n_iters = 200  # ğŸš€ å¤§å¹…å‡å°‘baselineè¯„ä¼°æ—¶é—´
        
        optimizer = torch.optim.Adam(
            baseline_model.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay
        )
        
        scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.9, min_lr=1e-6)
        
        # æ•°æ®åŠ è½½
        loader = DatasetLoader(dataset_fname)
        loader.split_train_val_test(train_ratio=0.8, val_ratio=0.1, seed=42)
        
        base_AP = [1]
        target_AP = [2]
        
        # è®­ç»ƒå¾ªç¯ - æ·»åŠ è¿›åº¦æ¡
        best_val_loss = float('inf')
        patience_counter = 0
        early_stopping_patience = 50  # ğŸš€ å¤§å¹…å‡å°‘æ—©åœè€å¿ƒ
        
        with tqdm(range(n_iters), desc="ğŸ”¬ Baseline Training", ncols=80) as pbar:
            for i in pbar:
                baseline_model.train()
                
                # è®­ç»ƒæ­¥éª¤
                sta_id = np.random.choice(loader.trainset, batch_size)
                sta_loc = torch.tensor(loader.get_loc_batch("STA", sta_id), device=device, dtype=torch.float32)
                
                input_cfr = torch.tensor(loader.get_cfr_batch(base_AP, sta_id).flatten(), device=device)
                input_cfr = torch.stack([torch.real(input_cfr), torch.imag(input_cfr)], dim=-1)
                
                target_cfr = torch.tensor(loader.get_cfr_batch(target_AP, sta_id).flatten(), device=device)
                
                output_cfr = baseline_model(sta_loc, input_cfr)
                loss = compute_mse_loss(output_cfr, target_cfr)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                # æ›´æ–°è¿›åº¦æ¡
                progress = int((i / n_iters) * 100)
                progress_tracker.update_baseline(progress)
                
                # éªŒè¯å’Œæ—©åœ
                if i % 100 == 0 or i == n_iters - 1:
                    baseline_model.eval()
                    with torch.no_grad():
                        val_sta_id = loader.valset
                        val_sta_loc = torch.tensor(loader.get_loc_batch("STA", val_sta_id), device=device, dtype=torch.float32)
                        
                        val_input_cfr = torch.tensor(loader.get_cfr_batch(base_AP, val_sta_id).flatten(), device=device)
                        val_input_cfr = torch.stack([torch.real(val_input_cfr), torch.imag(val_input_cfr)], dim=-1)
                        
                        val_target_cfr = torch.tensor(loader.get_cfr_batch(target_AP, val_sta_id).flatten(), device=device)
                        
                        val_output_cfr = baseline_model(val_sta_loc, val_input_cfr)
                        val_loss = compute_mse_loss(val_output_cfr, val_target_cfr)
                        
                        scheduler.step(val_loss)
                        
                        if val_loss.item() < best_val_loss:
                            best_val_loss = val_loss.item()
                            patience_counter = 0
                        else:
                            patience_counter += 1
                        
                        # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                        pbar.set_postfix({
                            'Loss': f'{loss.item():.6f}',
                            'ValLoss': f'{val_loss.item():.6f}',
                            'Best': f'{best_val_loss:.6f}',
                            'SNR': f'{compute_snr_db(best_val_loss):.1f}dB'
                        })
                        
                        if patience_counter >= early_stopping_patience:
                            logging.info(f"Baseline early stopping at iteration {i}")
                            break
                
                if i % 200 == 0:
                    torch.cuda.empty_cache()
        
        BASELINE_PERFORMANCE = {
            'loss': best_val_loss,
            'snr': compute_snr_db(best_val_loss),
            'model_params': sum(p.numel() for p in baseline_model.parameters())
        }
        
        progress_tracker.set_baseline_complete(best_val_loss)
        
        logging.info(f"âœ… Baseline model evaluation completed!")
        logging.info(f"   Validation Loss: {BASELINE_PERFORMANCE['loss']:.6f}")
        logging.info(f"   SNR: {BASELINE_PERFORMANCE['snr']:.2f} dB")
        logging.info(f"   Parameter Count: {BASELINE_PERFORMANCE['model_params']:,}")
        
        # æ¸…ç†å†…å­˜
        del baseline_model
        del optimizer
        del scheduler
        torch.cuda.empty_cache()
        
        return BASELINE_PERFORMANCE
        
    except Exception as e:
        logging.error(f"Baseline model evaluation failed: {e}")
        # Set a conservative baseline
        BASELINE_PERFORMANCE = {'loss': 1.0, 'snr': 0.0, 'model_params': 100000}
        progress_tracker.set_baseline_complete(1.0)
        return BASELINE_PERFORMANCE


class IntelligentPruner:
    """Intelligent pruner based on comparison with baseline"""
    
    def __init__(self, baseline_performance, improvement_threshold=0.02):
        self.baseline_loss = baseline_performance['loss']
        self.baseline_snr = baseline_performance['snr']
        self.improvement_threshold = improvement_threshold  # Minimum improvement threshold
        self.trial_history = []
        
    def should_prune(self, trial, step, intermediate_value):
        """Determine whether to prune"""
        
        # Record trial history
        if len(self.trial_history) >= step:
            self.trial_history[step - 1].append(intermediate_value)
        else:
            while len(self.trial_history) < step:
                self.trial_history.append([])
            self.trial_history[step - 1].append(intermediate_value)
        
        # Don't prune in early stages, give model enough training time
        if step < 5:
            return False
        
        # If current loss is much larger than baseline, consider pruning
        if intermediate_value > self.baseline_loss * (1 + self.improvement_threshold * 3):
            logging.info(f"ğŸ”ª Trial {trial.number} pruned: loss {intermediate_value:.6f} >> baseline {self.baseline_loss:.6f}")
            return True
        
        # If there's still no significant improvement in mid-to-late stages, prune
        if step >= 10:
            improvement = (self.baseline_loss - intermediate_value) / self.baseline_loss
            if improvement < -self.improvement_threshold:  # Negative improvement (worse)
                logging.info(f"ğŸ”ª Trial {trial.number} pruned: no improvement over baseline")
                return True
        
        # Dynamic pruning based on historical performance
        if step >= 15 and len(self.trial_history[step - 1]) >= 5:
            # Get historical best performance at the same step
            historical_best = min(self.trial_history[step - 1])
            if intermediate_value > historical_best * 1.2:  # 20% worse than historical best
                logging.info(f"ğŸ”ª Trial {trial.number} pruned: worse than historical best")
                return True
        
        return False

class OptimizedMLP(nn.Module):
    """å¯é…ç½®çš„ MLP æ¨¡å‹ï¼Œç”¨äºæ¶æ„æœç´¢ - æ¯å±‚ç‹¬ç«‹é…ç½®"""
    def __init__(self, trial, input_dim=12):  # é»˜è®¤ç»´åº¦æ”¹ä¸º12ï¼šSTA_loc(3) + input_cfr(9)
        super(OptimizedMLP, self).__init__()
        
        # å…¨å±€æ¶æ„å‚æ•°
        self.n_layers = trial.suggest_int('n_layers', 3, 8)  # ç½‘ç»œæ·±åº¦
        self.use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])
        self.init_method = trial.suggest_categorical('init_method', ['orthogonal'])
        self.skip_connections = trial.suggest_categorical('skip_connections', [False])
        
        # æ¯å±‚ç‹¬ç«‹çš„é…ç½®
        self.layer_configs = []
        self.layers = nn.ModuleList()
        self.batch_norms = nn.ModuleList() if self.use_batch_norm else None
        self.dropouts = nn.ModuleList()
        self.activations = []  # å­˜å‚¨æ¯å±‚çš„æ¿€æ´»å‡½æ•°ç±»å‹
        
        # å¯é€‰çš„éšè—å±‚å°ºå¯¸å’Œæ¿€æ´»å‡½æ•°
        hidden_dim_choices = [32, 64, 96, 128, 160, 192, 256]
        activation_choices = ['relu', 'leaky_relu', 'tanh', 'elu']
        dropout_choices = [0.0, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5]
        
        # ç¬¬ä¸€å±‚ï¼ˆè¾“å…¥å±‚åˆ°ç¬¬ä¸€ä¸ªéšè—å±‚ï¼‰
        first_hidden_dim = trial.suggest_categorical('layer_0_hidden_dim', hidden_dim_choices)
        first_activation = trial.suggest_categorical('layer_0_activation', activation_choices)
        first_dropout = trial.suggest_categorical('layer_0_dropout', dropout_choices)
        
        self.layer_configs.append({
            'hidden_dim': first_hidden_dim,
            'activation': first_activation,
            'dropout': first_dropout
        })
        self.activations.append(first_activation)
        
        # æ„å»ºç¬¬ä¸€å±‚
        self.layers.append(nn.Linear(input_dim, first_hidden_dim))
        if self.use_batch_norm:
            self.batch_norms.append(nn.BatchNorm1d(first_hidden_dim))
        self.dropouts.append(nn.Dropout(first_dropout))
        
        # æ„å»ºä¸­é—´éšè—å±‚
        prev_dim = first_hidden_dim
        for i in range(1, self.n_layers - 1):  # -1 å› ä¸ºè¿˜éœ€è¦è¾“å‡ºå±‚
            layer_hidden_dim = trial.suggest_categorical(f'layer_{i}_hidden_dim', hidden_dim_choices)
            layer_activation = trial.suggest_categorical(f'layer_{i}_activation', activation_choices)
            layer_dropout = trial.suggest_categorical(f'layer_{i}_dropout', dropout_choices)
            
            self.layer_configs.append({
                'hidden_dim': layer_hidden_dim,
                'activation': layer_activation,
                'dropout': layer_dropout
            })
            self.activations.append(layer_activation)
            
            # æ„å»ºå±‚
            self.layers.append(nn.Linear(prev_dim, layer_hidden_dim))
            if self.use_batch_norm:
                self.batch_norms.append(nn.BatchNorm1d(layer_hidden_dim))
            self.dropouts.append(nn.Dropout(layer_dropout))
            
            prev_dim = layer_hidden_dim
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(prev_dim, 2)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """æ ¹æ®æŒ‡å®šæ–¹æ³•åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                if self.init_method == 'he':
                    nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='relu')
                elif self.init_method == 'xavier':
                    nn.init.xavier_uniform_(module.weight)
                elif self.init_method == 'orthogonal':
                    nn.init.orthogonal_(module.weight)
                elif self.init_method == 'normal':
                    nn.init.normal_(module.weight, mean=0.0, std=0.1)
                
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)
    
    def _get_activation(self, activation_name):
        """è·å–æ¿€æ´»å‡½æ•°"""
        if activation_name == 'relu':
            return F.relu
        elif activation_name == 'leaky_relu':
            return F.leaky_relu
        elif activation_name == 'tanh':
            return torch.tanh
        elif activation_name == 'elu':
            return F.elu
        elif activation_name == 'swish':
            return lambda x: x * torch.sigmoid(x)
        elif activation_name == 'gelu':
            return F.gelu
        else:
            return F.relu
    
    def forward(self, STA_loc, input_cfr):
        # æ‹¼æ¥è¾“å…¥
        x = torch.cat([STA_loc, input_cfr], dim=-1)
        
        # å‰å‘ä¼ æ’­ - æ¯å±‚ä½¿ç”¨è‡ªå·±çš„æ¿€æ´»å‡½æ•°
        residual = None
        for i, layer in enumerate(self.layers):
            # è·³è·ƒè¿æ¥
            if self.skip_connections and i > 0 and residual is not None and residual.shape == x.shape:
                x = x + residual
            
            if self.skip_connections:
                residual = x
            
            # çº¿æ€§å˜æ¢
            x = layer(x)
            
            # BatchNorm
            if self.use_batch_norm and i < len(self.batch_norms):
                x = self.batch_norms[i](x)
            
            # æ¯å±‚ä½¿ç”¨è‡ªå·±çš„æ¿€æ´»å‡½æ•°
            activation_fn = self._get_activation(self.activations[i])
            x = activation_fn(x)
            
            # Dropout
            if i < len(self.dropouts):
                x = self.dropouts[i](x)
        
        # è¾“å‡ºå±‚
        x = self.output_layer(x)
        
        # è¾“å‡ºå¤æ•°å€¼
        re_ch = torch.tanh(x[..., 0])
        im_ch = torch.tanh(x[..., 1])
        output = re_ch + 1j * im_ch
        return output
    
    def get_architecture_info(self):
        """è·å–ç½‘ç»œæ¶æ„çš„è¯¦ç»†ä¿¡æ¯"""
        info = {
            'total_layers': self.n_layers,
            'use_batch_norm': self.use_batch_norm,
            'init_method': self.init_method,
            'skip_connections': self.skip_connections,
            'layer_configs': self.layer_configs,
            'total_params': sum(p.numel() for p in self.parameters()),
            'trainable_params': sum(p.numel() for p in self.parameters() if p.requires_grad)
        }
        return info
    
    def print_architecture(self):
        """æ‰“å°ç½‘ç»œæ¶æ„ä¿¡æ¯"""
        print("ğŸ—ï¸ Network Architecture:")
        print(f"   Total Layers: {self.n_layers}")
        print(f"   Skip Connections: {self.skip_connections}")
        print(f"   Batch Normalization: {self.use_batch_norm}")
        print(f"   Weight Initialization: {self.init_method}")
        print("   Layer Details:")
        
        for i, config in enumerate(self.layer_configs):
            print(f"     Layer {i}: {config['hidden_dim']} neurons, "
                  f"{config['activation']} activation, "
                  f"{config['dropout']:.2f} dropout")
        
        total_params = sum(p.numel() for p in self.parameters())
        print(f"   Total Parameters: {total_params:,}")


def display_best_architecture(best_params, stage="Final"):
    """æ˜¾ç¤ºæœ€ä½³æ¶æ„çš„è¯¦ç»†ä¿¡æ¯"""
    print("\n" + "=" * 70)
    print(f"ğŸ—ï¸ {stage} æœ€ä½³æ¶æ„è¯¦æƒ…")
    print("=" * 70)
    
    try:
        # åˆ›å»ºè™šæ‹Ÿtrialæ¥é‡å»ºæ¨¡å‹
        class DisplayTrial:
            def __init__(self, params):
                self.params = params
            def suggest_int(self, name, low, high):
                return self.params.get(name, (low + high) // 2)
            def suggest_float(self, name, low, high):
                return self.params.get(name, (low + high) / 2)
            def suggest_loguniform(self, name, low, high):
                return self.params.get(name, (low * high) ** 0.5)
            def suggest_categorical(self, name, choices):
                return self.params.get(name, choices[0])
        
        display_trial = DisplayTrial(best_params)
        model = OptimizedMLP(display_trial)
        
        # æ˜¾ç¤ºæ¶æ„ä¿¡æ¯
        model.print_architecture()
        
        # æ˜¾ç¤ºå‚æ•°åˆ†å¸ƒ
        print("\nğŸ“‹ å‚æ•°è¯¦ç»†åˆ†å¸ƒ:")
        architecture_params = {}
        hyperparams = {}
        
        for key, value in best_params.items():
            if any(x in key for x in ['layer_', 'n_layers', 'use_batch_norm', 'init_method', 'skip_connections']):
                architecture_params[key] = value
            else:
                hyperparams[key] = value
        
        if architecture_params:
            print("   ğŸ—ï¸ æ¶æ„å‚æ•°:")
            for key, value in sorted(architecture_params.items()):
                print(f"      {key}: {value}")
        
        if hyperparams:
            print("   âš™ï¸ è®­ç»ƒè¶…å‚æ•°:")
            for key, value in sorted(hyperparams.items()):
                print(f"      {key}: {value}")
                
    except Exception as e:
        logging.error(f"æ˜¾ç¤ºæ¶æ„ä¿¡æ¯æ—¶å‡ºé”™: {e}")


def compute_mse_loss(predicted, true):
    """è®¡ç®—ç›¸å¯¹MSEæŸå¤±"""
    return torch.sum(torch.abs(predicted - true) ** 2) / torch.sum(torch.abs(true) ** 2)


def compute_snr_db(loss):
    """è®¡ç®—SNR (dB)"""
    return -10.0 * np.log10(loss)


def objective_architecture(trial):
    """ç¬¬ä¸€é˜¶æ®µï¼šç½‘ç»œæ¶æ„æœç´¢ï¼ˆä¸baselineæ¯”è¾ƒï¼‰"""
    try:
        # è·å–baselineæ€§èƒ½ä½œä¸ºå‚è€ƒ
        baseline_perf = evaluate_baseline_model()
        
        # å›ºå®šè¶…å‚æ•°ï¼Œåªæœç´¢ç½‘ç»œæ¶æ„
        batch_size = 1024
        learning_rate = 5e-4
        weight_decay = 0
        scheduler_patience = 10
        scheduler_factor = 0.9
        train_split = 0.8
        val_split = 0.1
        n_iters = 200  # ğŸš€ å¤§å¹…å‡å°‘æ¶æ„æœç´¢æ—¶é—´
        
        # 2. æ•°æ®å’Œè®¾å¤‡è®¾ç½®
        dataset_fname = '/home/byang/BoYang/mNeWRF/dataset/conference_2000STA_4APs.pkl'
        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        
        # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
        torch.manual_seed(42)
        np.random.seed(42)
        random.seed(42)
        
        # 3. åˆ›å»ºæ¨¡å‹ï¼ˆåªæœç´¢æ¶æ„å‚æ•°ï¼‰
        model = OptimizedMLP(trial)
        model.to(device)
        
        # ğŸ” æ¨¡å‹å¤æ‚åº¦æ£€æŸ¥
        model_params = sum(p.numel() for p in model.parameters())
        baseline_params = baseline_perf['model_params']
        
        # å¦‚æœæ¨¡å‹è¿‡äºå¤æ‚ä½†æ²¡æœ‰æ˜æ˜¾æ”¹è¿›æ½œåŠ›ï¼Œæå‰å‰ªæ
        if model_params > baseline_params * 3:  # å‚æ•°é‡è¶…è¿‡baseline 3å€
            logging.info(f"ğŸ”ª Trial {trial.number} pruned: too complex ({model_params:,} vs {baseline_params:,} params)")
            raise optuna.TrialPruned()
        
        # 4. ä¼˜åŒ–å™¨è®¾ç½®
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay
        )
        
        scheduler = ReduceLROnPlateau(
            optimizer, 'min', 
            patience=scheduler_patience, 
            factor=scheduler_factor, 
            min_lr=1e-6
        )
        
        # 5. æ•°æ®åŠ è½½
        loader = DatasetLoader(dataset_fname)
        loader.split_train_val_test(
            train_ratio=train_split, 
            val_ratio=val_split, 
            seed=42
        )
        
        base_AP = [1]
        target_AP = [2]
        
        # è®­ç»ƒå¾ªç¯ - æ·»åŠ è¿›åº¦æ¡
        best_val_loss = float('inf')
        patience_counter = 0
        early_stopping_patience = 50  # ğŸš€ å‡å°‘æ¶æ„æœç´¢æ—©åœè€å¿ƒ
        
        # ğŸ“Š æ€§èƒ½è¿½è¸ª
        val_losses_history = []
        
        with tqdm(range(n_iters), desc=f"ğŸ—ï¸ Trial{trial.number:3d}", ncols=100, leave=False) as pbar:
            for i in pbar:
                model.train()
                
                # éšæœºé‡‡æ ·è®­ç»ƒæ•°æ®
                sta_id = np.random.choice(loader.trainset, batch_size)
                sta_loc = torch.tensor(loader.get_loc_batch("STA", sta_id), device=device, dtype=torch.float32)
                
                input_cfr = torch.tensor(loader.get_cfr_batch(base_AP, sta_id).flatten(), device=device)
                input_cfr = torch.stack([torch.real(input_cfr), torch.imag(input_cfr)], dim=-1)
                
                target_cfr = torch.tensor(loader.get_cfr_batch(target_AP, sta_id).flatten(), device=device)
                
                # å‰å‘ä¼ æ’­
                output_cfr = model(sta_loc, input_cfr)
                loss = compute_mse_loss(output_cfr, target_cfr)
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                # éªŒè¯å’Œå‰ªææ£€æŸ¥
                if i % 100 == 0 or i == n_iters - 1:
                    model.eval()
                    with torch.no_grad():
                        val_sta_id = loader.valset
                        val_sta_loc = torch.tensor(loader.get_loc_batch("STA", val_sta_id), device=device, dtype=torch.float32)
                        
                        val_input_cfr = torch.tensor(loader.get_cfr_batch(base_AP, val_sta_id).flatten(), device=device)
                        val_input_cfr = torch.stack([torch.real(val_input_cfr), torch.imag(val_input_cfr)], dim=-1)
                        
                        val_target_cfr = torch.tensor(loader.get_cfr_batch(target_AP, val_sta_id).flatten(), device=device)
                        
                        val_output_cfr = model(val_sta_loc, val_input_cfr)
                        val_loss = compute_mse_loss(val_output_cfr, val_target_cfr)
                        
                        val_snr = compute_snr_db(val_loss.item())
                        val_losses_history.append(val_loss.item())
                        
                        # å­¦ä¹ ç‡è°ƒåº¦
                        scheduler.step(val_loss)
                        
                        # æ—©åœæ£€æŸ¥
                        if val_loss.item() < best_val_loss:
                            best_val_loss = val_loss.item()
                            patience_counter = 0
                        else:
                            patience_counter += 1
                        
                        # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                        improvement = (baseline_perf['loss'] - val_loss.item()) / baseline_perf['loss'] * 100
                        pbar.set_postfix({
                            'Loss': f'{val_loss.item():.6f}',
                            'Best': f'{best_val_loss:.6f}',
                            'SNR': f'{val_snr:.1f}dB',
                            'Î”': f'{improvement:+.1f}%',
                            'Params': f'{model_params//1000}k'
                        })
                        
                        if patience_counter >= early_stopping_patience:
                            logging.info(f"Early stopping at iteration {i}")
                            break
                        
                        # æŠ¥å‘Šä¸­é—´ç»“æœç»™ Optuna
                        step = i // 100 + 1
                        trial.report(val_loss.item(), step)
                        
                        # ğŸ”ª æ™ºèƒ½å‰ªæï¼šä¸baselineæ¯”è¾ƒ
                        if step >= 3:  # ç»™æ¨¡å‹ä¸€äº›è®­ç»ƒæ—¶é—´
                            # å¦‚æœè¿ç»­å¤šæ­¥éƒ½æ¯”baselineå·®å¾ˆå¤šï¼Œå‰ªæ
                            recent_losses = val_losses_history[-3:] if len(val_losses_history) >= 3 else val_losses_history
                            avg_recent_loss = sum(recent_losses) / len(recent_losses)
                            
                            improvement = (baseline_perf['loss'] - avg_recent_loss) / baseline_perf['loss']
                            
                            if improvement < -0.05:  # æ¯”baselineå·®5%ä»¥ä¸Š
                                logging.info(f"ğŸ”ª Trial {trial.number} pruned: worse than baseline by {abs(improvement)*100:.1f}%")
                                raise optuna.TrialPruned()
                        
                        # æ ‡å‡†å‰ªææ£€æŸ¥
                        if trial.should_prune():
                            raise optuna.TrialPruned()
                
                # å®šæœŸæ¸…ç†GPUå†…å­˜
                if i % 200 == 0:
                    torch.cuda.empty_cache()
        
        # 7. æœ€ç»ˆæ€§èƒ½è¯„ä¼°
        final_improvement = (baseline_perf['loss'] - best_val_loss) / baseline_perf['loss']
        final_snr = compute_snr_db(best_val_loss)
        
        logging.info(f"ğŸ¯ Architecture trial {trial.number} completed:")
        logging.info(f"   Val Loss: {best_val_loss:.6f} (baseline: {baseline_perf['loss']:.6f})")
        logging.info(f"   SNR: {final_snr:.2f} dB (baseline: {baseline_perf['snr']:.2f} dB)")
        logging.info(f"   Improvement: {final_improvement*100:.2f}%")
        logging.info(f"   Params: {model_params:,} (baseline: {baseline_params:,})")
        
        # æ¸…ç†å†…å­˜
        del model
        del optimizer
        del scheduler
        torch.cuda.empty_cache()
        
        return best_val_loss
    
    except optuna.TrialPruned:
        # æ¸…ç†å†…å­˜
        torch.cuda.empty_cache()
        raise
    except Exception as e:
        logging.error(f"Architecture trial failed with error: {e}")
        # æ¸…ç†å†…å­˜
        torch.cuda.empty_cache()
        return float('inf')


def objective_hyperparams(trial, best_architecture_params, baseline_perf):
    """ç¬¬äºŒé˜¶æ®µï¼šè¶…å‚æ•°æœç´¢ï¼ˆåŸºäºæœ€ä½³æ¶æ„å’Œbaselineæ¯”è¾ƒï¼‰"""
    try:
        # 1. ä½¿ç”¨æœ€ä½³æ¶æ„ï¼Œæœç´¢è¶…å‚æ•°
        batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024, 2048, 4096])
        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
        weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)
        scheduler_patience = trial.suggest_int('scheduler_patience', 5, 20)
        scheduler_factor = trial.suggest_float('scheduler_factor', 0.1, 0.9)
        
        # å›ºå®šæ•°æ®åˆ’åˆ†å’Œè¿­ä»£æ¬¡æ•°
        train_split = 0.8
        val_split = 0.1
        n_iters = 800  # ğŸš€ å‡å°‘è¶…å‚æ•°æœç´¢æ—¶é—´
        
        # 2. æ•°æ®å’Œè®¾å¤‡è®¾ç½®
        dataset_fname = '/home/byang/BoYang/mNeWRF/dataset/conference_2000STA_4APs.pkl'
        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        
        # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
        torch.manual_seed(42)
        np.random.seed(42)
        random.seed(42)
        
        # 3. ä½¿ç”¨æœ€ä½³æ¶æ„åˆ›å»ºæ¨¡å‹
        class FakeTrial:
            def __init__(self, params):
                self.params = params
            
            def suggest_int(self, name, low, high):
                return self.params.get(name, (low + high) // 2)
            
            def suggest_float(self, name, low, high):
                return self.params.get(name, (low + high) / 2)
            
            def suggest_loguniform(self, name, low, high):
                return self.params.get(name, (low * high) ** 0.5)
            
            def suggest_categorical(self, name, choices):
                return self.params.get(name, choices[0])
        
        fake_trial = FakeTrial(best_architecture_params)
        model = OptimizedMLP(fake_trial)
        model.to(device)
        
        # 4. ä¼˜åŒ–å™¨è®¾ç½®ï¼ˆä½¿ç”¨æœç´¢çš„è¶…å‚æ•°ï¼‰
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay
        )
        
        scheduler = ReduceLROnPlateau(
            optimizer, 'min', 
            patience=scheduler_patience, 
            factor=scheduler_factor, 
            min_lr=1e-6
        )
        
        # 5. æ•°æ®åŠ è½½
        loader = DatasetLoader(dataset_fname)
        loader.split_train_val_test(
            train_ratio=train_split, 
            val_ratio=val_split, 
            seed=42
        )
        
        base_AP = [1]
        target_AP = [2]
        
        # è®­ç»ƒå¾ªç¯ - æ·»åŠ è¿›åº¦æ¡
        best_val_loss = float('inf')
        patience_counter = 0
        early_stopping_patience = 80  # ğŸš€ å‡å°‘è¶…å‚æ•°æœç´¢æ—©åœè€å¿ƒ
        
        with tqdm(range(n_iters), desc=f"âš¡HP{trial.number:3d}", ncols=100, leave=False) as pbar:
            for i in pbar:
                model.train()
                
                # éšæœºé‡‡æ ·è®­ç»ƒæ•°æ®
                sta_id = np.random.choice(loader.trainset, batch_size)
                sta_loc = torch.tensor(loader.get_loc_batch("STA", sta_id), device=device, dtype=torch.float32)
                
                input_cfr = torch.tensor(loader.get_cfr_batch(base_AP, sta_id).flatten(), device=device)
                input_cfr = torch.stack([torch.real(input_cfr), torch.imag(input_cfr)], dim=-1)
                
                target_cfr = torch.tensor(loader.get_cfr_batch(target_AP, sta_id).flatten(), device=device)
                
                # å‰å‘ä¼ æ’­
                output_cfr = model(sta_loc, input_cfr)
                loss = compute_mse_loss(output_cfr, target_cfr)
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                # éªŒè¯å’Œæ—©åœæ£€æŸ¥
                if i % 100 == 0 or i == n_iters - 1:
                    model.eval()
                    with torch.no_grad():
                        val_sta_id = loader.valset
                        val_sta_loc = torch.tensor(loader.get_loc_batch("STA", val_sta_id), device=device, dtype=torch.float32)
                        
                        val_input_cfr = torch.tensor(loader.get_cfr_batch(base_AP, val_sta_id).flatten(), device=device)
                        val_input_cfr = torch.stack([torch.real(val_input_cfr), torch.imag(val_input_cfr)], dim=-1)
                        
                        val_target_cfr = torch.tensor(loader.get_cfr_batch(target_AP, val_sta_id).flatten(), device=device)
                        
                        val_output_cfr = model(val_sta_loc, val_input_cfr)
                        val_loss = compute_mse_loss(val_output_cfr, val_target_cfr)
                        
                        val_snr = compute_snr_db(val_loss.item())
                        
                        # å­¦ä¹ ç‡è°ƒåº¦
                        scheduler.step(val_loss)
                        
                        # æ—©åœæ£€æŸ¥
                        if val_loss.item() < best_val_loss:
                            best_val_loss = val_loss.item()
                            patience_counter = 0
                        else:
                            patience_counter += 1
                        
                        # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                        improvement = (baseline_perf['loss'] - val_loss.item()) / baseline_perf['loss'] * 100
                        pbar.set_postfix({
                            'Loss': f'{val_loss.item():.6f}',
                            'Best': f'{best_val_loss:.6f}',
                            'SNR': f'{val_snr:.1f}dB',
                            'Î”': f'{improvement:+.1f}%',
                            'LR': f'{optimizer.param_groups[0]["lr"]:.0e}'
                        })
                        
                        if patience_counter >= early_stopping_patience:
                            logging.info(f"Early stopping at iteration {i}")
                            break
                        
                        # æŠ¥å‘Šä¸­é—´ç»“æœç»™ Optuna
                        trial.report(val_loss.item(), i)
                        
                        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‰ªæ
                        if trial.should_prune():
                            raise optuna.TrialPruned()
                
                # å®šæœŸæ¸…ç†GPUå†…å­˜
                if i % 200 == 0:
                    torch.cuda.empty_cache()
        
        # 7. æœ€ç»ˆæ€§èƒ½è¯„ä¼°ä¸baselineæ¯”è¾ƒ
        final_improvement = (baseline_perf['loss'] - best_val_loss) / baseline_perf['loss']
        final_snr = compute_snr_db(best_val_loss)
        
        logging.info(f"ğŸ¯ Hyperparameter trial {trial.number} completed:")
        logging.info(f"   Val Loss: {best_val_loss:.6f} (baseline: {baseline_perf['loss']:.6f})")
        logging.info(f"   SNR: {final_snr:.2f} dB (baseline: {baseline_perf['snr']:.2f} dB)")
        logging.info(f"   Total Improvement: {final_improvement*100:.2f}%")
        
        # æ¸…ç†å†…å­˜
        del model
        del optimizer
        del scheduler
        torch.cuda.empty_cache()
        
        return best_val_loss
    
    except optuna.TrialPruned:
        # æ¸…ç†å†…å­˜
        torch.cuda.empty_cache()
        raise
    except Exception as e:
        logging.error(f"Hyperparameter trial failed with error: {e}")
        # æ¸…ç†å†…å­˜
        torch.cuda.empty_cache()
        return float('inf')
    
    except Exception as e:
        logging.error(f"Trial failed with error: {e}")
        # æ¸…ç†å†…å­˜
        torch.cuda.empty_cache()
        return float('inf')


def create_progress_plot(save_path='nas_progress.png'):
    """Create NAS progress visualization charts"""
    try:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('NAS Progress Monitor', fontsize=16, fontweight='bold')
        
        # Subplot 1: Architecture search progress
        if len(progress_tracker.arch_losses) > 0:
            trials = list(range(1, len(progress_tracker.arch_losses) + 1))
            ax1.plot(trials, progress_tracker.arch_losses, 'b-', marker='o', markersize=4, alpha=0.7, label='Architecture Search')
            if progress_tracker.baseline_loss:
                ax1.axhline(y=progress_tracker.baseline_loss, color='r', linestyle='--', alpha=0.7, label='Baseline')
            ax1.set_xlabel('Trial Number')
            ax1.set_ylabel('Validation Loss')
            ax1.set_title('Architecture Search Progress')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            ax1.set_yscale('log')
        else:
            ax1.text(0.5, 0.5, 'Architecture Search Not Started', ha='center', va='center', transform=ax1.transAxes, fontsize=12)
            ax1.set_title('Architecture Search Progress')
        
        # Subplot 2: Hyperparameter search progress
        if len(progress_tracker.hp_losses) > 0:
            trials = list(range(1, len(progress_tracker.hp_losses) + 1))
            ax2.plot(trials, progress_tracker.hp_losses, 'g-', marker='s', markersize=4, alpha=0.7, label='Hyperparameter Search')
            if progress_tracker.baseline_loss:
                ax2.axhline(y=progress_tracker.baseline_loss, color='r', linestyle='--', alpha=0.7, label='Baseline')
            ax2.set_xlabel('Trial Number')
            ax2.set_ylabel('Validation Loss')
            ax2.set_title('Hyperparameter Search Progress')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            ax2.set_yscale('log')
        else:
            ax2.text(0.5, 0.5, 'Hyperparameter Search Not Started', ha='center', va='center', transform=ax2.transAxes, fontsize=12)
            ax2.set_title('Hyperparameter Search Progress')
        
        # Subplot 3: Overall progress bars
        stages = ['Baseline', 'Architecture Search', 'Hyperparameter Search']
        progress_values = [
            100 if progress_tracker.baseline_loss else progress_tracker.baseline_progress,
            (progress_tracker.arch_trials_completed / progress_tracker.arch_trials_total * 100) if progress_tracker.arch_trials_total > 0 else 0,
            (progress_tracker.hp_trials_completed / progress_tracker.hp_trials_total * 100) if progress_tracker.hp_trials_total > 0 else 0
        ]
        colors = ['red', 'blue', 'green']
        bars = ax3.barh(stages, progress_values, color=colors, alpha=0.7)
        ax3.set_xlim(0, 100)
        ax3.set_xlabel('Progress (%)')
        ax3.set_title('Overall Progress')
        
        # Show percentage on each progress bar
        for bar, value in zip(bars, progress_values):
            width = bar.get_width()
            ax3.text(width/2, bar.get_y() + bar.get_height()/2, f'{value:.1f}%', 
                    ha='center', va='center', fontweight='bold', color='white')
        
        # Subplot 4: Performance comparison
        if progress_tracker.baseline_loss:
            categories = ['Baseline']
            losses = [progress_tracker.baseline_loss]
            colors_bar = ['red']
            
            if progress_tracker.arch_best_loss < float('inf'):
                categories.append('Best Architecture')
                losses.append(progress_tracker.arch_best_loss)
                colors_bar.append('blue')
            
            if progress_tracker.hp_best_loss < float('inf'):
                categories.append('Best Hyperparams')
                losses.append(progress_tracker.hp_best_loss)
                colors_bar.append('green')
            
            bars = ax4.bar(categories, losses, color=colors_bar, alpha=0.7)
            ax4.set_ylabel('Validation Loss')
            ax4.set_title('Performance Comparison')
            ax4.set_yscale('log')
            
            # Show specific values and improvement percentages
            for i, (bar, loss) in enumerate(zip(bars, losses)):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height*1.05,
                        f'{loss:.6f}', ha='center', va='bottom', fontweight='bold')
                
                if i > 0:  # Calculate improvement relative to baseline
                    improvement = (progress_tracker.baseline_loss - loss) / progress_tracker.baseline_loss * 100
                    ax4.text(bar.get_x() + bar.get_width()/2., height*0.5,
                            f'{improvement:+.1f}%', ha='center', va='center', 
                            color='white', fontweight='bold')
        else:
            ax4.text(0.5, 0.5, 'Waiting for Results...', ha='center', va='center', transform=ax4.transAxes, fontsize=12)
            ax4.set_title('Performance Comparison')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logging.info(f"ğŸ“Š Progress chart saved: {save_path}")
        
    except Exception as e:
        logging.error(f"Failed to create progress chart: {e}")


def should_save_progress_plot():
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿å­˜è¿›åº¦å›¾ - åªåœ¨å…³é”®è¿›åº¦ç‚¹ä¿å­˜"""
    # è®¡ç®—æ€»ä½“è¿›åº¦
    total_progress = 0
    completed_stages = 0
    
    # Baselineé˜¶æ®µ
    if progress_tracker.baseline_loss is not None:
        completed_stages += 1
        total_progress += 100
    else:
        total_progress += progress_tracker.baseline_progress
    
    # æ¶æ„æœç´¢é˜¶æ®µ
    if progress_tracker.arch_trials_total > 0:
        arch_progress = (progress_tracker.arch_trials_completed / progress_tracker.arch_trials_total) * 100
        total_progress += arch_progress
        if arch_progress >= 100:
            completed_stages += 1
    
    # è¶…å‚æ•°æœç´¢é˜¶æ®µ  
    if progress_tracker.hp_trials_total > 0:
        hp_progress = (progress_tracker.hp_trials_completed / progress_tracker.hp_trials_total) * 100
        total_progress += hp_progress
        if hp_progress >= 100:
            completed_stages += 1
    
    # è®¡ç®—å¹³å‡è¿›åº¦
    expected_stages = 3  # baseline + arch + hp
    overall_progress = total_progress / expected_stages
    
    # å®šä¹‰å…³é”®è¿›åº¦ç‚¹ (10%, 20%, 30%, ..., 90%, 100%)
    key_milestones = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
    
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ–°çš„å…³é”®ç‚¹
    if not hasattr(should_save_progress_plot, 'last_saved_milestone'):
        should_save_progress_plot.last_saved_milestone = 0
    
    for milestone in key_milestones:
        if (overall_progress >= milestone and 
            should_save_progress_plot.last_saved_milestone < milestone):
            should_save_progress_plot.last_saved_milestone = milestone
            return True, milestone
    
    return False, 0


def cleanup_old_progress_plots(keep_recent=3):
    """æ¸…ç†æ—§çš„è¿›åº¦å›¾ï¼Œåªä¿ç•™æœ€è¿‘çš„å‡ å¼ """
    try:
        import glob
        import os
        
        # æ‰¾åˆ°æ‰€æœ‰è¿›åº¦å›¾æ–‡ä»¶ (åŒ¹é… nas_progress_XX%_HHMMSS.png æ ¼å¼)
        progress_files = glob.glob('nas_progress_*%_*.png')
        
        if len(progress_files) > keep_recent:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
            progress_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
            
            # åˆ é™¤æ—§çš„æ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°çš„ keep_recent å¼ 
            files_to_delete = progress_files[keep_recent:]
            for file_path in files_to_delete:
                try:
                    os.remove(file_path)
                    logging.info(f"ğŸ—‘ï¸  Cleaned up old progress chart: {file_path}")
                except Exception as e:
                    logging.warning(f"Failed to delete {file_path}: {e}")
                    
    except Exception as e:
        logging.warning(f"Progress chart cleanup failed: {e}")


def periodic_progress_update():
    """å®šæœŸæ›´æ–°è¿›åº¦å¹¶åœ¨å…³é”®ç‚¹åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    while True:
        try:
            progress_tracker.print_status()
            
            # åªåœ¨å…³é”®è¿›åº¦ç‚¹ä¿å­˜å›¾ç‰‡
            should_save, milestone = should_save_progress_plot()
            if should_save:
                timestamp = datetime.now().strftime("%H%M%S")
                save_path = f'nas_progress_{milestone}%_{timestamp}.png'
                create_progress_plot(save_path)
                logging.info(f"ğŸ“Š Progress milestone {milestone}% reached! Chart saved: {save_path}")
                
                # æ¸…ç†æ—§çš„è¿›åº¦å›¾
                cleanup_old_progress_plots(keep_recent=5)
            
            time.sleep(30)  # Update every 30 seconds
        except Exception as e:
            logging.error(f"Progress update failed: {e}")
            time.sleep(30)


def run_two_stage_nas(architecture_trials=30, hyperparams_trials=50, storage_url=None):
    """è¿è¡Œä¸¤é˜¶æ®µ NASï¼šç¬¬ä¸€é˜¶æ®µæœç´¢æ¶æ„ï¼Œç¬¬äºŒé˜¶æ®µæœç´¢è¶…å‚æ•°"""
    
    # é‡ç½®è¿›åº¦è¿½è¸ªå™¨
    progress_tracker.reset()
    progress_tracker.start_time = time.time()
    
    # æ˜¾ç¤ºå›¾ç‰‡ä¿å­˜ç­–ç•¥
    logging.info("ğŸ“Š å›¾ç‰‡ä¿å­˜ç­–ç•¥: åªåœ¨å…³é”®è¿›åº¦ç‚¹ä¿å­˜ (10%, 20%, 30%, ..., 90%, 100%)")
    logging.info("ğŸ—‘ï¸  è‡ªåŠ¨æ¸…ç†: ä¿ç•™æœ€è¿‘5å¼ è¿›åº¦å›¾ï¼Œè‡ªåŠ¨åˆ é™¤æ—§å›¾ç‰‡")
    
    # å¯åŠ¨åå°è¿›åº¦æ›´æ–°çº¿ç¨‹
    progress_thread = threading.Thread(target=periodic_progress_update, daemon=True)
    progress_thread.start()
    
    if storage_url is None:
        storage_url = "sqlite:///two_stage_nas.db"
    
    # ğŸ¯ é¦–å…ˆè¯„ä¼°baselineæ€§èƒ½
    baseline_perf = evaluate_baseline_model()
    
    # ======================= ç¬¬ä¸€é˜¶æ®µï¼šæ¶æ„æœç´¢ =======================
    logging.info("ğŸ—ï¸  å¼€å§‹ç¬¬ä¸€é˜¶æ®µï¼šç¥ç»ç½‘ç»œæ¶æ„æœç´¢")
    logging.info(f"ğŸ¯ Baselineæ€§èƒ½ - Loss: {baseline_perf['loss']:.6f}, SNR: {baseline_perf['snr']:.2f} dB")
    logging.info(f"ç›®æ ‡è¯•éªŒæ•°: {architecture_trials}")
    
    # æ›´æ–°è¿›åº¦è¿½è¸ªå™¨
    progress_tracker.set_arch_stage(architecture_trials)
    
    # åˆ›å»ºæ™ºèƒ½å‰ªæå™¨
    intelligent_pruner = IntelligentPruner(baseline_perf, improvement_threshold=0.02)
    
    # åˆ›å»ºæ¶æ„æœç´¢ç ”ç©¶
    arch_study = optuna.create_study(
        direction='minimize',
        storage=storage_url,
        study_name='architecture_search',
        load_if_exists=True,
        pruner=optuna.pruners.MedianPruner(
            n_startup_trials=5,
            n_warmup_steps=3,
            interval_steps=3  # æ›´é¢‘ç¹çš„å‰ªææ£€æŸ¥
        ),
        sampler=optuna.samplers.TPESampler(
            n_startup_trials=8,
            multivariate=True
        )
    )
    
    # è‡ªå®šä¹‰å›è°ƒå‡½æ•°æ¥è¿½è¸ªè¿›åº¦
    def arch_callback(study, trial):
        if trial.state == optuna.trial.TrialState.COMPLETE:
            progress_tracker.update_arch_trial(len(study.trials), study.best_value)
            logging.info(f"ğŸ—ï¸  æ¶æ„è¯•éªŒ {trial.number+1}/{architecture_trials} å®Œæˆ | æœ€ä½³æŸå¤±: {study.best_value:.6f}")
    
    # æ‰§è¡Œæ¶æ„æœç´¢
    try:
        with tqdm(total=architecture_trials, desc="ğŸ—ï¸ æ¶æ„æœç´¢", ncols=80) as pbar:
            def wrapped_objective(trial):
                result = objective_architecture(trial)
                pbar.update(1)
                return result
            
            arch_study.optimize(wrapped_objective, n_trials=architecture_trials, callbacks=[arch_callback])
    except KeyboardInterrupt:
        logging.info("æ¶æ„æœç´¢è¢«ç”¨æˆ·ä¸­æ–­")
    
    # åˆ†ææ¶æ„æœç´¢ç»“æœ
    completed_trials = [t for t in arch_study.trials if t.state == optuna.trial.TrialState.COMPLETE]
    pruned_trials = [t for t in arch_study.trials if t.state == optuna.trial.TrialState.PRUNED]
    
    if not completed_trials:
        logging.error("âŒ æ²¡æœ‰å®Œæˆçš„æ¶æ„æœç´¢è¯•éªŒï¼")
        logging.info("å°è¯•é™ä½æœç´¢æ ‡å‡†æˆ–å¢åŠ è¯•éªŒæ•°...")
        return None, None, None
    
    # è·å–æœ€ä½³æ¶æ„
    best_arch_params = arch_study.best_params
    best_arch_loss = arch_study.best_value
    
    # æ£€æŸ¥æ˜¯å¦æ¯”baselineæ›´å¥½
    improvement = (baseline_perf['loss'] - best_arch_loss) / baseline_perf['loss']
    
    logging.info("=" * 60)
    logging.info("ğŸ¯ ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼æ¶æ„æœç´¢ç»“æœ:")
    logging.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_arch_loss:.6f}")
    logging.info(f"æœ€ä½³ SNR: {compute_snr_db(best_arch_loss):.2f} dB")
    logging.info(f"ç›¸æ¯”Baselineæ”¹è¿›: {improvement*100:.2f}%")
    logging.info(f"å®Œæˆè¯•éªŒ: {len(completed_trials)}, å‰ªæè¯•éªŒ: {len(pruned_trials)}")
    
    if improvement <= 0:
        logging.warning("âš ï¸  æœ€ä½³æ¶æ„æœªè¶…è¶Šbaselineï¼Œè€ƒè™‘:")
        logging.warning("   1. å¢åŠ æ¶æ„æœç´¢è¯•éªŒæ•°")
        logging.warning("   2. æ‰©å¤§æœç´¢ç©ºé—´")
        logging.warning("   3. è°ƒæ•´å‰ªæç­–ç•¥")
    else:
        logging.info(f"âœ… æ‰¾åˆ°æ›´å¥½çš„æ¶æ„ï¼æ”¹è¿› {improvement*100:.2f}%")
    
    logging.info("æœ€ä½³æ¶æ„å‚æ•°:")
    for key, value in best_arch_params.items():
        logging.info(f"  {key}: {value}")
    
    # ä¿å­˜æœ€ä½³æ¶æ„
    with open('best_architecture.pkl', 'wb') as f:
        pickle.dump(best_arch_params, f)
    logging.info("æœ€ä½³æ¶æ„å·²ä¿å­˜åˆ° 'best_architecture.pkl'")
    
    # ======================= ç¬¬äºŒé˜¶æ®µï¼šè¶…å‚æ•°æœç´¢ =======================
    logging.info("\n" + "=" * 60)
    logging.info("âš¡ å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šè¶…å‚æ•°æœç´¢")
    logging.info(f"ä½¿ç”¨æœ€ä½³æ¶æ„ï¼Œç›®æ ‡è¯•éªŒæ•°: {hyperparams_trials}")
    
    # æ›´æ–°è¿›åº¦è¿½è¸ªå™¨
    progress_tracker.set_hp_stage(hyperparams_trials)
    
    # åˆ›å»ºè¶…å‚æ•°æœç´¢ç ”ç©¶
    hp_study = optuna.create_study(
        direction='minimize',
        storage=storage_url,
        study_name='hyperparams_search',
        load_if_exists=True,
        pruner=optuna.pruners.MedianPruner(
            n_startup_trials=10,
            n_warmup_steps=5,
            interval_steps=10
        ),
        sampler=optuna.samplers.TPESampler(
            n_startup_trials=15,
            multivariate=True
        )
    )
    
    # åˆ›å»ºå¸¦æœ‰æœ€ä½³æ¶æ„çš„ç›®æ ‡å‡½æ•°
    def objective_with_best_arch(trial):
        return objective_hyperparams(trial, best_arch_params, baseline_perf)
    
    # è‡ªå®šä¹‰å›è°ƒå‡½æ•°æ¥è¿½è¸ªè¶…å‚æ•°æœç´¢è¿›åº¦
    def hp_callback(study, trial):
        if trial.state == optuna.trial.TrialState.COMPLETE:
            progress_tracker.update_hp_trial(len(study.trials), study.best_value)
            logging.info(f"âš¡ è¶…å‚æ•°è¯•éªŒ {trial.number+1}/{hyperparams_trials} å®Œæˆ | æœ€ä½³æŸå¤±: {study.best_value:.6f}")
    
    # æ‰§è¡Œè¶…å‚æ•°æœç´¢
    try:
        with tqdm(total=hyperparams_trials, desc="âš¡ è¶…å‚æ•°æœç´¢", ncols=80) as pbar:
            def wrapped_hp_objective(trial):
                result = objective_with_best_arch(trial)
                pbar.update(1)
                return result
            
            hp_study.optimize(wrapped_hp_objective, n_trials=hyperparams_trials, callbacks=[hp_callback])
    except KeyboardInterrupt:
        logging.info("è¶…å‚æ•°æœç´¢è¢«ç”¨æˆ·ä¸­æ–­")
    
    # è·å–æœ€ç»ˆæœ€ä½³ç»“æœ
    hp_completed_trials = [t for t in hp_study.trials if t.state == optuna.trial.TrialState.COMPLETE]
    
    if not hp_completed_trials:
        logging.warning("âš ï¸  è¶…å‚æ•°æœç´¢æ— å®Œæˆè¯•éªŒï¼Œä½¿ç”¨æ¶æ„æœç´¢ç»“æœ")
        final_best_params = best_arch_params
        final_best_loss = best_arch_loss
    else:
        best_hp_params = hp_study.best_params
        best_hp_loss = hp_study.best_value
        final_best_params = {**best_arch_params, **best_hp_params}
        final_best_loss = best_hp_loss
    
    # æœ€ç»ˆç»“æœåˆ†æ
    final_improvement = (baseline_perf['loss'] - final_best_loss) / baseline_perf['loss']
    
    logging.info("=" * 60)
    logging.info("ğŸ† ä¸¤é˜¶æ®µæœç´¢å®Œæˆï¼æœ€ç»ˆç»“æœ:")
    logging.info(f"Baselineæ€§èƒ½  - Loss: {baseline_perf['loss']:.6f}, SNR: {baseline_perf['snr']:.2f} dB")
    logging.info(f"æœ€ç»ˆæœ€ä½³æ€§èƒ½ - Loss: {final_best_loss:.6f}, SNR: {compute_snr_db(final_best_loss):.2f} dB")
    logging.info(f"æ€»ä½“æ”¹è¿›: {final_improvement*100:.2f}%")
    
    if final_improvement > 0:
        logging.info(f"ğŸ‰ æˆåŠŸæ‰¾åˆ°æ›´å¥½çš„æ¨¡å‹é…ç½®ï¼")
    else:
        logging.warning(f"âš ï¸  æœ€ç»ˆç»“æœæœªè¶…è¶Šbaseline")
    
    if len(hp_completed_trials) > 0:
        arch_improvement = (baseline_perf['loss'] - best_arch_loss) / baseline_perf['loss']
        hp_improvement = (best_arch_loss - final_best_loss) / best_arch_loss if best_arch_loss > 0 else 0
        logging.info(f"æ¶æ„è´¡çŒ®: {arch_improvement*100:.2f}%")
        logging.info(f"è¶…å‚æ•°è´¡çŒ®: {hp_improvement*100:.2f}%")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    with open('final_best_params.pkl', 'wb') as f:
        pickle.dump(final_best_params, f)
    logging.info("\næœ€ç»ˆæœ€ä½³å‚æ•°å·²ä¿å­˜åˆ° 'final_best_params.pkl'")
    
    # æœç´¢ç»Ÿè®¡
    logging.info("\n" + "=" * 60)
    logging.info("ğŸ“Š æœç´¢ç»Ÿè®¡:")
    logging.info(f"æ¶æ„æœç´¢ - å®Œæˆ: {len(completed_trials)}, å‰ªæ: {len(pruned_trials)}")
    logging.info(f"è¶…å‚æ•°æœç´¢ - å®Œæˆ: {len(hp_completed_trials)}, å‰ªæ: {len([t for t in hp_study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
    
    # åˆ›å»ºæœ€ç»ˆç»“æœå¯è§†åŒ–
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    final_chart_path = f'final_nas_results_{timestamp}.png'
    create_progress_plot(final_chart_path)
    logging.info(f"ğŸ“Š æœ€ç»ˆç»“æœå›¾è¡¨å·²ä¿å­˜: {final_chart_path}")
    
    return arch_study, hp_study, final_best_params


def run_optuna_search(n_trials=100, storage_url=None):
    """è¿è¡Œä¸¤é˜¶æ®µ Optuna æœç´¢çš„å…¥å£å‡½æ•°"""
    # å°†æ€»è¯•éªŒæ•°åˆ†é…ç»™ä¸¤ä¸ªé˜¶æ®µ
    architecture_trials = min(30, n_trials // 3)  # 1/3 ç”¨äºæ¶æ„æœç´¢
    hyperparams_trials = n_trials - architecture_trials  # 2/3 ç”¨äºè¶…å‚æ•°æœç´¢
    
    return run_two_stage_nas(architecture_trials, hyperparams_trials, storage_url)


def load_and_test_best_model(best_params_file='final_best_params.pkl'):
    """ä½¿ç”¨æœ€ä½³å‚æ•°åˆ›å»ºå’Œæµ‹è¯•æ¨¡å‹"""
    logging.info("åŠ è½½æœ€ä½³å‚æ•°å¹¶æµ‹è¯•æ¨¡å‹...")
    
    with open(best_params_file, 'rb') as f:
        best_params = pickle.load(f)
    
    # åˆ›å»ºä¸€ä¸ªä¼ª trial å¯¹è±¡ç”¨äºæ¨¡å‹æ„å»º
    class FakeTrial:
        def __init__(self, params):
            self.params = params
        
        def suggest_int(self, name, low, high):
            return self.params.get(name, (low + high) // 2)
        
        def suggest_float(self, name, low, high):
            return self.params.get(name, (low + high) / 2)
        
        def suggest_loguniform(self, name, low, high):
            return self.params.get(name, (low * high) ** 0.5)
        
        def suggest_categorical(self, name, choices):
            return self.params.get(name, choices[0])
    
    fake_trial = FakeTrial(best_params)
    
    # åˆ›å»ºæœ€ä½³æ¨¡å‹
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    model = OptimizedMLP(fake_trial)
    model.to(device)
    
    logging.info("ğŸ† æœ€ç»ˆæœ€ä½³æ¨¡å‹é…ç½®:")
    logging.info("æ¶æ„å‚æ•°:")
    arch_params = ['n_layers', 'hidden_dim', 'dropout_rate', 'activation', 'use_batch_norm', 'init_method', 'skip_connections']
    for param in arch_params:
        if param in best_params:
            logging.info(f"  {param}: {best_params[param]}")
    
    logging.info("è¶…å‚æ•°:")
    hp_params = ['batch_size', 'learning_rate', 'weight_decay', 'scheduler_patience', 'scheduler_factor']
    for param in hp_params:
        if param in best_params:
            logging.info(f"  {param}: {best_params[param]}")
    
    return model, best_params


if __name__ == "__main__":
    # è¿è¡Œä¸¤é˜¶æ®µ NAS
    logging.info("ğŸš€ å¼€å§‹ä¸¤é˜¶æ®µç¥ç»æ¶æ„æœç´¢")
    
    # ç¬¬ä¸€é˜¶æ®µï¼šæ¶æ„æœç´¢ (30 trials)
    # ç¬¬äºŒé˜¶æ®µï¼šè¶…å‚æ•°æœç´¢ (70 trials) 
    arch_study, hp_study, final_params = run_two_stage_nas(
        architecture_trials=30,
        hyperparams_trials=70
    )
    
    # æµ‹è¯•æœ€ä½³æ¨¡å‹
    try:
        best_model, best_params = load_and_test_best_model('final_best_params.pkl')
        logging.info("ğŸ¯ æœ€ä½³æ¨¡å‹åˆ›å»ºæˆåŠŸï¼")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in best_model.parameters())
        trainable_params = sum(p.numel() for p in best_model.parameters() if p.requires_grad)
        logging.info(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")
        logging.info(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¹Ÿæ˜¾ç¤ºå®ƒçš„æ¶æ„ä¿¡æ¯
        if hasattr(best_model, 'print_architecture'):
            best_model.print_architecture()
        
        # æ˜¾ç¤ºè¯¦ç»†çš„æœ€ä½³æ¶æ„ä¿¡æ¯
        display_best_architecture(best_params, "Final Best")
        
    except Exception as e:
        logging.error(f"åˆ›å»ºæœ€ä½³æ¨¡å‹å¤±è´¥: {e}")
        
    logging.info("âœ… ä¸¤é˜¶æ®µ NAS æœç´¢å®Œæˆï¼")
