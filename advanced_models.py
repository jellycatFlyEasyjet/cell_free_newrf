import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class MLP_Advanced(nn.Module):
    """
    å¸¦æœ‰é«˜çº§æƒé‡åˆå§‹åŒ–é€‰é¡¹çš„MLPæ¨¡å‹
    """
    def __init__(self, input_dim=3 + 2, hidden_dim=128, output_dim=2, 
                 init_method='he', dropout_rate=0.0, use_batch_norm=False):
        super(MLP_Advanced, self).__init__()
        
        self.init_method = init_method
        self.use_batch_norm = use_batch_norm
        
        self.layer1 = nn.Linear(input_dim, hidden_dim)
        self.layer2 = nn.Linear(hidden_dim, hidden_dim)
        self.layer3 = nn.Linear(hidden_dim, hidden_dim)
        self.layer4 = nn.Linear(hidden_dim, hidden_dim)
        self.layer5 = nn.Linear(hidden_dim, hidden_dim)
        self.layer6 = nn.Linear(hidden_dim, output_dim)
        
        # å¯é€‰çš„æ‰¹é‡å½’ä¸€åŒ–å±‚
        if use_batch_norm:
            self.bn1 = nn.BatchNorm1d(hidden_dim)
            self.bn2 = nn.BatchNorm1d(hidden_dim)
            self.bn3 = nn.BatchNorm1d(hidden_dim)
            self.bn4 = nn.BatchNorm1d(hidden_dim)
            self.bn5 = nn.BatchNorm1d(hidden_dim)
        
        # å¯é€‰çš„Dropoutå±‚
        if dropout_rate > 0:
            self.dropout = nn.Dropout(dropout_rate)
        else:
            self.dropout = None
        
        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        """
        é«˜çº§æƒé‡åˆå§‹åŒ–æ–¹æ³•
        æ”¯æŒå¤šç§åˆå§‹åŒ–ç­–ç•¥
        """
        print(f"ğŸ”§ ä½¿ç”¨ '{self.init_method}' æ–¹æ³•åˆå§‹åŒ–æƒé‡...")
        
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                if self.init_method == 'he' or self.init_method == 'kaiming':
                    # He/Kaimingåˆå§‹åŒ– - é€‚åˆReLUæ¿€æ´»å‡½æ•°
                    if module == self.layer6:  # è¾“å‡ºå±‚
                        nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='linear')
                        gain = 0.5  # å¯¹è¾“å‡ºå±‚ä½¿ç”¨è¾ƒå°çš„gain
                        with torch.no_grad():
                            module.weight.data *= gain
                    else:
                        nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='relu')
                
                elif self.init_method == 'xavier' or self.init_method == 'glorot':
                    # Xavier/Glorotåˆå§‹åŒ– - é€‚åˆtanh/sigmoidæ¿€æ´»å‡½æ•°
                    if module == self.layer6:
                        nn.init.xavier_uniform_(module.weight, gain=0.5)
                    else:
                        nn.init.xavier_uniform_(module.weight, gain=1.0)
                
                elif self.init_method == 'normal':
                    # æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
                    std = 0.02 if module == self.layer6 else 0.1
                    nn.init.normal_(module.weight, mean=0.0, std=std)
                
                elif self.init_method == 'orthogonal':
                    # æ­£äº¤åˆå§‹åŒ– - æœ‰åŠ©äºæ¢¯åº¦æµåŠ¨
                    gain = 0.5 if module == self.layer6 else 1.0
                    nn.init.orthogonal_(module.weight, gain=gain)
                
                elif self.init_method == 'nerf_default':
                    # NeRFè®ºæ–‡ä¸­å¸¸ç”¨çš„åˆå§‹åŒ–æ–¹æ³•
                    if module == self.layer6:
                        # è¾“å‡ºå±‚ä½¿ç”¨é›¶åˆå§‹åŒ–
                        nn.init.constant_(module.weight, 0.0)
                    else:
                        # éšè—å±‚ä½¿ç”¨æ­£æ€åˆ†å¸ƒ
                        nn.init.normal_(module.weight, mean=0.0, std=0.1)
                
                # åç½®åˆå§‹åŒ–
                if module.bias is not None:
                    if self.init_method == 'nerf_default' and module == self.layer6:
                        # NeRFè¾“å‡ºå±‚åç½®åˆå§‹åŒ–ä¸ºå°æ­£å€¼
                        nn.init.constant_(module.bias, 0.1)
                    else:
                        nn.init.constant_(module.bias, 0.0)
        
        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
        self._print_weight_stats()
    
    def _print_weight_stats(self):
        """æ‰“å°æƒé‡ç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“Š æƒé‡åˆå§‹åŒ–ç»Ÿè®¡:")
        for name, param in self.named_parameters():
            if 'weight' in name:
                mean = param.data.mean().item()
                std = param.data.std().item()
                print(f"  {name}: mean={mean:.6f}, std={std:.6f}, shape={list(param.shape)}")
    
    def forward(self, STA_loc, input_cfr):
        # è¿æ¥è¾“å…¥
        input_x = torch.cat([STA_loc, input_cfr], dim=-1)
        x = input_x
        
        # å‰å‘ä¼ æ’­
        x = F.relu(self.layer1(x))
        if self.use_batch_norm:
            x = self.bn1(x)
        if self.dropout:
            x = self.dropout(x)
        
        x = F.relu(self.layer2(x))
        if self.use_batch_norm:
            x = self.bn2(x)
        if self.dropout:
            x = self.dropout(x)
        
        x = F.relu(self.layer3(x))
        if self.use_batch_norm:
            x = self.bn3(x)
        if self.dropout:
            x = self.dropout(x)
        
        x = F.relu(self.layer4(x))
        if self.use_batch_norm:
            x = self.bn4(x)
        if self.dropout:
            x = self.dropout(x)
        
        x = F.relu(self.layer5(x))
        if self.use_batch_norm:
            x = self.bn5(x)
        if self.dropout:
            x = self.dropout(x)
        
        # è¾“å‡ºå±‚
        x = self.layer6(x)
        re_ch = torch.tanh(x[..., 0])
        im_ch = torch.tanh(x[..., 1])
        output = re_ch + 1j * im_ch
        return output

# æƒé‡åˆå§‹åŒ–å·¥å…·å‡½æ•°
def initialize_model_weights(model, method='he'):
    """
    ä¸ºç°æœ‰æ¨¡å‹åˆå§‹åŒ–æƒé‡çš„å·¥å…·å‡½æ•°
    
    Args:
        model: PyTorchæ¨¡å‹
        method: åˆå§‹åŒ–æ–¹æ³• ('he', 'xavier', 'normal', 'orthogonal', 'nerf_default')
    """
    print(f"ğŸ”§ ä½¿ç”¨ '{method}' æ–¹æ³•åˆå§‹åŒ–æ¨¡å‹æƒé‡...")
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            if method == 'he':
                nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='relu')
            elif method == 'xavier':
                nn.init.xavier_uniform_(module.weight)
            elif method == 'normal':
                nn.init.normal_(module.weight, mean=0.0, std=0.1)
            elif method == 'orthogonal':
                nn.init.orthogonal_(module.weight)
            elif method == 'nerf_default':
                if 'layer6' in name:  # è¾“å‡ºå±‚
                    nn.init.constant_(module.weight, 0.0)
                else:
                    nn.init.normal_(module.weight, mean=0.0, std=0.1)
            
            if module.bias is not None:
                nn.init.constant_(module.bias, 0.0)
    
    print("âœ… æƒé‡åˆå§‹åŒ–å®Œæˆ")

if __name__ == "__main__":
    # æµ‹è¯•ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•
    print("="*60)
    print("ğŸ§ª æµ‹è¯•ä¸åŒçš„æƒé‡åˆå§‹åŒ–æ–¹æ³•")
    print("="*60)
    
    methods = ['he', 'xavier', 'normal', 'orthogonal', 'nerf_default']
    
    for method in methods:
        print(f"\nğŸ”¬ æµ‹è¯• {method} åˆå§‹åŒ–:")
        model = MLP_Advanced(init_method=method)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size = 4
        sta_loc = torch.randn(batch_size, 3)
        input_cfr = torch.randn(batch_size, 2)
        
        with torch.no_grad():
            output = model(sta_loc, input_cfr)
            print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"  è¾“å‡ºå‡å€¼: {output.abs().mean().item():.6f}")
            print(f"  è¾“å‡ºæ ‡å‡†å·®: {output.abs().std().item():.6f}")
