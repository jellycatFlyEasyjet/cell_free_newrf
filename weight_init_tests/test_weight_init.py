#!/usr/bin/env python3
"""
æµ‹è¯•ä¸åŒæƒé‡åˆå§‹åŒ–æ–¹æ³•å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import sys
import os

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ° Python è·¯å¾„ä»¥å¯¼å…¥ä¸»æ¨¡å—
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)

from models import MLP
from advanced_models import MLP_Advanced, initialize_model_weights

def test_weight_initialization_impact():
    """æµ‹è¯•æƒé‡åˆå§‹åŒ–å¯¹è®­ç»ƒçš„å½±å“"""
    
    print("="*80)
    print("ğŸ§ª æµ‹è¯•æƒé‡åˆå§‹åŒ–å¯¹è®­ç»ƒçš„å½±å“")
    print("="*80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    batch_size = 32
    n_samples = 1000
    
    # æ¨¡æ‹ŸSTAä½ç½®æ•°æ® (x, y, zåæ ‡)
    sta_locs = torch.randn(n_samples, 3) * 2.0  # åœ¨[-2, 2]èŒƒå›´å†…
    
    # æ¨¡æ‹Ÿè¾“å…¥CFRæ•°æ® (å®éƒ¨, è™šéƒ¨)
    input_cfrs = torch.randn(n_samples, 2) * 0.1  # å°å¹…åº¦çš„CFR
    
    # ç”Ÿæˆç›®æ ‡CFRæ•°æ® (åŸºäºä½ç½®çš„ç®€å•å‡½æ•°)
    target_real = 0.01 * torch.sin(sta_locs[:, 0]) * torch.cos(sta_locs[:, 1])
    target_imag = 0.01 * torch.cos(sta_locs[:, 0]) * torch.sin(sta_locs[:, 1])
    targets = target_real + 1j * target_imag
    
    # æµ‹è¯•ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•
    methods = {
        'default': lambda: MLP(),  # ä½¿ç”¨æˆ‘ä»¬æ”¹è¿›çš„MLP
        'xavier_advanced': lambda: MLP_Advanced(init_method='xavier'),
        'he_advanced': lambda: MLP_Advanced(init_method='he'),
        'nerf_advanced': lambda: MLP_Advanced(init_method='nerf_default'),
        'orthogonal_advanced': lambda: MLP_Advanced(init_method='orthogonal')
    }
    
    results = {}
    
    for method_name, model_creator in methods.items():
        print(f"\nğŸ”¬ æµ‹è¯• {method_name} åˆå§‹åŒ–æ–¹æ³•:")
        
        # åˆ›å»ºæ¨¡å‹
        model = model_creator()
        model.to(device)
        
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        sta_locs_dev = sta_locs.to(device)
        input_cfrs_dev = input_cfrs.to(device)
        targets_dev = targets.to(device)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        # æŸå¤±å‡½æ•°
        def compute_loss(pred, target):
            return torch.sum(torch.abs(pred - target) ** 2) / torch.sum(torch.abs(target) ** 2)
        
        # è®­ç»ƒå‡ ä¸ªepochæ¥æµ‹è¯•åˆå§‹æ”¶æ•›æ€§
        n_epochs = 50
        losses = []
        
        model.train()
        for epoch in range(n_epochs):
            # éšæœºé‡‡æ ·batch
            indices = torch.randperm(n_samples)[:batch_size]
            batch_sta_locs = sta_locs_dev[indices]
            batch_input_cfrs = input_cfrs_dev[indices]
            batch_targets = targets_dev[indices]
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            outputs = model(batch_sta_locs, batch_input_cfrs)
            loss = compute_loss(outputs, batch_targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            losses.append(loss.item())
            
            if epoch % 10 == 0:
                print(f"  Epoch {epoch:2d}: Loss = {loss.item():.6f}")
        
        results[method_name] = {
            'losses': losses,
            'final_loss': losses[-1],
            'initial_loss': losses[0],
            'convergence_rate': (losses[0] - losses[-1]) / losses[0] if losses[0] > 0 else 0
        }
        
        print(f"  æœ€ç»ˆæŸå¤±: {losses[-1]:.6f}")
        print(f"  æ”¶æ•›ç‡: {results[method_name]['convergence_rate']*100:.1f}%")
    
    # å¯è§†åŒ–ç»“æœ
    plt.figure(figsize=(15, 10))
    
    # æŸå¤±æ›²çº¿å¯¹æ¯”
    plt.subplot(2, 2, 1)
    for method_name, result in results.items():
        plt.plot(result['losses'], label=method_name, linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss Comparison')
    plt.legend()
    plt.yscale('log')
    plt.grid(True, alpha=0.3)
    
    # æœ€ç»ˆæŸå¤±å¯¹æ¯”
    plt.subplot(2, 2, 2)
    methods_list = list(results.keys())
    final_losses = [results[method]['final_loss'] for method in methods_list]
    colors = plt.cm.Set3(np.linspace(0, 1, len(methods_list)))
    
    bars = plt.bar(methods_list, final_losses, color=colors)
    plt.xlabel('Initialization Method')
    plt.ylabel('Final Loss')
    plt.title('Final Loss Comparison')
    plt.xticks(rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, loss in zip(bars, final_losses):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                f'{loss:.4f}', ha='center', va='bottom', fontsize=10)
    
    # æ”¶æ•›ç‡å¯¹æ¯”
    plt.subplot(2, 2, 3)
    convergence_rates = [results[method]['convergence_rate']*100 for method in methods_list]
    
    bars = plt.bar(methods_list, convergence_rates, color=colors)
    plt.xlabel('Initialization Method')
    plt.ylabel('Convergence Rate (%)')
    plt.title('Convergence Rate Comparison')
    plt.xticks(rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, rate in zip(bars, convergence_rates):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                f'{rate:.1f}%', ha='center', va='bottom', fontsize=10)
    
    # åˆå§‹vsæœ€ç»ˆæŸå¤±å¯¹æ¯”
    plt.subplot(2, 2, 4)
    initial_losses = [results[method]['initial_loss'] for method in methods_list]
    
    x = np.arange(len(methods_list))
    width = 0.35
    
    bars1 = plt.bar(x - width/2, initial_losses, width, label='Initial Loss', alpha=0.7)
    bars2 = plt.bar(x + width/2, final_losses, width, label='Final Loss', alpha=0.7)
    
    plt.xlabel('Initialization Method')
    plt.ylabel('Loss')
    plt.title('Initial vs Final Loss')
    plt.xticks(x, methods_list, rotation=45)
    plt.legend()
    plt.yscale('log')
    
    plt.tight_layout()
    plt.savefig('/home/byang/BoYang/mNeWRF/weight_init_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # æ‰“å°æ€»ç»“
    print(f"\n" + "="*80)
    print("ğŸ“Š æƒé‡åˆå§‹åŒ–æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    # æ‰¾åˆ°æœ€ä½³æ–¹æ³•
    best_method = min(results.keys(), key=lambda x: results[x]['final_loss'])
    fastest_convergence = max(results.keys(), key=lambda x: results[x]['convergence_rate'])
    
    print(f"ğŸ† æœ€ä½³æœ€ç»ˆæŸå¤±: {best_method} (æŸå¤±: {results[best_method]['final_loss']:.6f})")
    print(f"ğŸš€ æœ€å¿«æ”¶æ•›: {fastest_convergence} (æ”¶æ•›ç‡: {results[fastest_convergence]['convergence_rate']*100:.1f}%)")
    
    print(f"\nğŸ“ˆ è¯¦ç»†ç»“æœ:")
    for method_name, result in results.items():
        print(f"  {method_name:20s}: æœ€ç»ˆæŸå¤±={result['final_loss']:.6f}, æ”¶æ•›ç‡={result['convergence_rate']*100:.1f}%")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    if best_method == fastest_convergence:
        print(f"  æ¨èä½¿ç”¨ '{best_method}' åˆå§‹åŒ–æ–¹æ³•ï¼Œå®ƒåœ¨æœ€ç»ˆæŸå¤±å’Œæ”¶æ•›é€Ÿåº¦ä¸Šéƒ½è¡¨ç°æœ€ä½³")
    else:
        print(f"  å¦‚æœè¿½æ±‚æœ€ä½æŸå¤±ï¼Œä½¿ç”¨ '{best_method}' åˆå§‹åŒ–")
        print(f"  å¦‚æœè¿½æ±‚å¿«é€Ÿæ”¶æ•›ï¼Œä½¿ç”¨ '{fastest_convergence}' åˆå§‹åŒ–")
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼å›¾è¡¨å·²ä¿å­˜ä¸º 'weight_init_comparison.png'")

if __name__ == "__main__":
    test_weight_initialization_impact()
