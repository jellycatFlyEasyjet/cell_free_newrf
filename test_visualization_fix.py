#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¿®å¤åçš„è®­ç»ƒå¯è§†åŒ–
"""

import sys
import os
sys.path.append('/home/byang/BoYang/mNeWRF')

import torch
import numpy as np
import matplotlib.pyplot as plt

def test_visualization_fix():
    """æµ‹è¯•ä¿®å¤åçš„å¯è§†åŒ–ä»£ç """
    print("="*60)
    print("ğŸ§ª æµ‹è¯•è®­ç»ƒå¯è§†åŒ–ä¿®å¤")
    print("="*60)
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    n_iters = 50
    train_losses = []
    val_losses = []
    train_losses_raw = []
    val_losses_raw = []
    SNRs = []
    val_SNRs = []
    iternum = []
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆæ¨¡æ‹Ÿå®é™…è®­ç»ƒä¸­çš„æŸå¤±å˜åŒ–ï¼‰
    base_train = 1.0
    base_val = 0.8
    
    for i in range(n_iters):
        # è®­ç»ƒæŸå¤±ï¼šé€æ¸å‡å°ä½†æœ‰å™ªå£°
        train_loss = base_train * np.exp(-i/20) + 0.1 * np.random.random()
        train_loss_raw = train_loss * 0.5 + 0.01 * np.random.random()
        
        # éªŒè¯æŸå¤±ï¼šç±»ä¼¼ä½†ç¨ä¸åŒçš„å°ºåº¦
        val_loss = base_val * np.exp(-i/25) + 0.05 * np.random.random()
        val_loss_raw = val_loss * 0.4 + 0.008 * np.random.random()
        
        train_losses.append(train_loss)
        train_losses_raw.append(train_loss_raw)
        
        if i % 5 == 0:  # éªŒè¯æŸå¤±ä¸æ˜¯æ¯æ¬¡éƒ½è®¡ç®—
            val_losses.append(val_loss)
            val_losses_raw.append(val_loss_raw)
            val_SNRs.append(-10 * np.log10(val_loss))
            iternum.append(i)
        
        SNRs.append(-10 * np.log10(train_loss))
    
    # æ¨¡æ‹ŸéªŒè¯æ•°æ®
    val_output_cfr = torch.randn(32) * 0.1 + 0.1j * torch.randn(32)
    val_target_cfr = torch.randn(32) * 0.1 + 0.1j * torch.randn(32)
    val_SNR = -2.5
    
    i = n_iters - 1
    
    print("ğŸ“Š ç”Ÿæˆçš„æµ‹è¯•æ•°æ®ç»Ÿè®¡:")
    print(f"  è®­ç»ƒæŸå¤±èŒƒå›´: {min(train_losses):.4f} - {max(train_losses):.4f}")
    print(f"  éªŒè¯æŸå¤±èŒƒå›´: {min(val_losses):.4f} - {max(val_losses):.4f}")
    print(f"  åŸå§‹è®­ç»ƒæŸå¤±èŒƒå›´: {min(train_losses_raw):.4f} - {max(train_losses_raw):.4f}")
    print(f"  åŸå§‹éªŒè¯æŸå¤±èŒƒå›´: {min(val_losses_raw):.4f} - {max(val_losses_raw):.4f}")
    
    # æµ‹è¯•å¯è§†åŒ–ä»£ç 
    print("\nğŸ¨ ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–å›¾è¡¨...")
    
    # ä½¿ç”¨ä¿®å¤åçš„å¯è§†åŒ–ä»£ç 
    fig, ax = plt.subplots(2,3, figsize=(18, 10))
    
    # ç¬¬ä¸€è¡Œç¬¬ä¸€ä¸ªï¼šç›¸å¯¹æŸå¤±å¯¹æ¯”
    ax[0,0].plot(range(len(train_losses)), train_losses, 'b-', label='Train Loss (Relative)', alpha=0.7, linewidth=1.5)
    ax[0,0].plot(range(len(val_losses)), val_losses, 'r-', label='Val Loss (Relative)', alpha=0.7, linewidth=1.5)
    ax[0,0].set_xlabel('Iterations')
    ax[0,0].set_ylabel('Relative Loss')
    ax[0,0].set_yscale('log')
    ax[0,0].legend()
    ax[0,0].grid(True, alpha=0.3)
    ax[0,0].set_title('Relative Loss (Used for Training)')

    # ç¬¬ä¸€è¡Œç¬¬äºŒä¸ªï¼šåŸå§‹MSEæŸå¤±å¯¹æ¯”
    ax[0,1].plot(range(len(train_losses_raw)), train_losses_raw, 'b-', label='Train Loss (Raw MSE)', alpha=0.7, linewidth=1.5)
    ax[0,1].plot(range(len(val_losses_raw)), val_losses_raw, 'r-', label='Val Loss (Raw MSE)', alpha=0.7, linewidth=1.5)
    ax[0,1].set_xlabel('Iterations')
    ax[0,1].set_ylabel('Raw MSE Loss')
    ax[0,1].set_yscale('log')
    ax[0,1].legend()
    ax[0,1].grid(True, alpha=0.3)
    ax[0,1].set_title('Raw MSE Loss (Same Scale)')

    # ç¬¬ä¸€è¡Œç¬¬ä¸‰ä¸ªï¼šSNRå¯¹æ¯”
    ax[0,2].plot(range(0, i + 1), SNRs, 'r-', label="Train SNR", alpha=0.7, linewidth=1.5)
    ax[0,2].plot(iternum, val_SNRs, 'y-', label='Val SNR', alpha=0.7, linewidth=1.5)
    ax[0,2].set_xlabel('Iterations')
    ax[0,2].set_ylabel('SNR (dB)')
    ax[0,2].legend()
    ax[0,2].grid(True, alpha=0.3)
    ax[0,2].set_title('SNR Progress')

    # ç¬¬äºŒè¡Œç¬¬ä¸€ä¸ªï¼šå¤æ•°é¢„æµ‹å¯è§†åŒ–
    ax[1,0].plot(np.real(val_output_cfr), np.imag(val_output_cfr), "ro", label="prediction", alpha=0.7, markersize=3)
    ax[1,0].plot(np.real(val_target_cfr), np.imag(val_target_cfr), "bo", label="Target", alpha=0.7, markersize=3)
    ax[1,0].set_xlabel('Real Part')
    ax[1,0].set_ylabel('Imaginary Part')
    ax[1,0].set_title(f"AP_[2] predict SNR: {val_SNR:.2f} dB")
    ax[1,0].legend()
    ax[1,0].grid(True, alpha=0.3)

    # ç¬¬äºŒè¡Œç¬¬äºŒä¸ªï¼šæŸå¤±å·®å¼‚
    if len(train_losses_raw) > 0 and len(val_losses_raw) > 0:
        loss_diff = np.array(train_losses_raw) - np.array(val_losses_raw[:len(train_losses_raw)])
        ax[1,1].plot(range(len(loss_diff)), loss_diff, 'g-', label='Train - Val Loss', alpha=0.7, linewidth=1.5)
        ax[1,1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
        ax[1,1].set_xlabel('Iterations')
        ax[1,1].set_ylabel('Loss Difference')
        ax[1,1].legend()
        ax[1,1].grid(True, alpha=0.3)
        ax[1,1].set_title('Train-Val Loss Difference')

    # ç¬¬äºŒè¡Œç¬¬ä¸‰ä¸ªï¼šå­¦ä¹ ç‡ï¼ˆæ¨¡æ‹Ÿï¼‰
    current_lr = 5e-4
    ax[1,2].axhline(y=current_lr, color='purple', linewidth=2, label=f'Current LR: {current_lr:.2e}')
    ax[1,2].set_xlabel('Iterations')
    ax[1,2].set_ylabel('Learning Rate')
    ax[1,2].legend()
    ax[1,2].grid(True, alpha=0.3)
    ax[1,2].set_title('Learning Rate Schedule')

    plt.tight_layout()
    plt.savefig('/home/byang/BoYang/mNeWRF/test_visualization_fix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("\nâœ… å¯è§†åŒ–æµ‹è¯•å®Œæˆ!")
    print("ğŸ“ˆ å…³é”®æ”¹è¿›:")
    print("  1. ä¿®å¤äº† ax[3] ç´¢å¼•é”™è¯¯ï¼ˆç°åœ¨ä½¿ç”¨2x3å¸ƒå±€ï¼‰")
    print("  2. æ·»åŠ äº†åŸå§‹MSEæŸå¤±æ˜¾ç¤ºï¼ˆç›¸åŒå°ºåº¦ï¼‰")
    print("  3. æ·»åŠ äº†æŸå¤±å·®å¼‚åˆ†æ")
    print("  4. æ”¹è¿›äº†å›¾è¡¨å¸ƒå±€å’Œå¯è¯»æ€§")
    print("  5. æ·»åŠ äº†å­¦ä¹ ç‡ç›‘æ§")
    
    return True

if __name__ == "__main__":
    test_visualization_fix()
    
    print("\n" + "="*60)
    print("ğŸ¯ è®­ç»ƒå¯è§†åŒ–ä¿®å¤å®Œæˆï¼")
    print("="*60)
    print("ğŸ’¡ ä¸»è¦è§£å†³æ–¹æ¡ˆ:")
    print("  1. ä½¿ç”¨ 2x3 å­å›¾å¸ƒå±€ä»£æ›¿ 1x3")
    print("  2. åˆ†åˆ«æ˜¾ç¤ºç›¸å¯¹æŸå¤±å’ŒåŸå§‹MSEæŸå¤±")
    print("  3. åŸå§‹MSEæŸå¤±ä½¿ç”¨ç›¸åŒè®¡ç®—æ–¹å¼ï¼Œå°ºåº¦ä¸€è‡´")
    print("  4. æ·»åŠ è¯¦ç»†çš„æŸå¤±å¯¹æ¯”åˆ†æ")
    print("\nâœ… ç°åœ¨ train loss å’Œ val loss å¯ä»¥åœ¨ç›¸åŒå°ºåº¦ä¸‹è§‚å¯Ÿäº†ï¼")
